{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TitleTop"
      },
      "source": [
        "\n",
        "\n",
        "# Pixel Art Diffusion v2.01 by [KaliYuga](https://twitter.com/KaliYuga_ai)\n",
        "\n",
        "Pixel Art Diffusion is a custom-trained unconditional diffusion model trained on an original (small) dataset of ~4100 256x256 pixel art landscapes using the [fine-tuning openai diffusion model](https://colab.research.google.com/drive/1Xfd5fm4OnhTd6IHPMGcoqw54uhGT3HdF) notebook by [Alex Spirin](https://twitter.com/devdef). I will expand this dataset over time. \n",
        "\n",
        "Pixel Art Diffusion V.2 runs within a fork of [Disco Diffusion 5.2 Warp](https://github.com/Sxela/DiscoDiffusion-Warp/blob/main/Disco_Diffusion_v5_2_Warp_custom_model.ipynb) notebook by [Alex Spirin](https://twitter.com/devdef).\n",
        "\n",
        "-------\n",
        "## What's New in V.2?\n",
        "Version 2 includes a whole bunch of siginificant updates. \n",
        "\n",
        "*   It's the debeut of a brand new, 3x larger pixel art model, which improves and diversifies output. It also should lessen the need to prompt with \"#pixelart\" over and over, although this may still be the power move for more complex prompts.  This is the new default model and settings are updated accordingly. You can still use the older models if you like! They're in the dropdown menu.\n",
        "*   Integrated Zippy's [ColorFx kit](https://colab.research.google.com/github/zippy731/disco-diffusion-turbo/blob/skunk/Disco_Diffusion.ipynb?), which adds a whole lot of new animation and color tools. These are as follows:\n",
        " - `soft_limiter` - reduce clipping at the end of each diffusion\n",
        " - `skip_end_steps` / frames_skip_end_steps - image too crunchy? maybe stop a few steps early!\n",
        " - `sat_scale_buffer` - improves sat scale function to reduce clipping directly in in the diffusion sat_scale loss function\n",
        " - `noise_injector` - (3D anim) injects color-matched noise into dead spots in anim frames\n",
        " - `copy_palette` - (3D anim) lock animation frames to a color palette from a given image\n",
        " - `show_histogram` - analyze color mix of image partials to diagnose individual images\n",
        "\n",
        "- Added support for the secondary model. It's not enabled by default because it seems to want to turn everything into buildings, which is irritating. However, it might be super useful for architectural prompts/game level making/etc. As always, ymmv so I thought I'd give you the option!\n",
        "-----\n",
        "\n",
        "## About Pixel Art Diffusion's Native Models (Updated for V2)\n",
        " There are actually three separate models within Pixel Art Diffusion as of the V2.0 release.\n",
        " \n",
        "**Models 1 & 2:\"Hard\" and \"Soft\"** \n",
        "\n",
        "You can select either in the drop-down **diffusion_model** menu. The underlying datasets (~1500 256x256 images) are identical, but the training steps and original checkpoints I trained each from are different, and in my testing, I’ve found that they’re good at different things. \n",
        "Soft is probably my favorite of the two, if I had to pick. I think it’s more diverse and successful in its outputs. It’s better at “soft” pixel art vibes and colors, although it can be made crunchier by tweaking settings some. Hard is crunchier by default and also more saturated. \n",
        "\n",
        "In V1, Pixel Art Diffusion’s default settings were tuned for the \"Soft\" Pixel Art Diffusion model.\n",
        "The only settings that vary from V2's default settings are:\n",
        "\n",
        "- **Clip_model Selection:** ViTb32+ViTb16+ViTl14+ RN50 + RN50x4 | [More about steps on the Ez Charts wiki](https://ezcharts.miraheze.org/wiki/Category:CLIP_Model_selectors)\n",
        "\n",
        "- **steps:** 100 | [More about Clip_model selectors on the Ez Charts wiki](https://ezcharts.miraheze.org/wiki/Category:Steps)\n",
        "\n",
        "- **skip_steps:** 10 | [More about skip_steps on the Ez Charts wiki](https://ezcharts.miraheze.org/wiki/Category:Skip_steps)\n",
        "\n",
        "- **eta:** 1 | [More about eta on the Ez Charts wiki](https://ezcharts.miraheze.org/wiki/Category:Eta)\n",
        "\n",
        "- **perlin_init:** true \n",
        "- **perlin_mode:** color [More about Perlin stuff on the Ez Charts wiki](https://ezcharts.miraheze.org/wiki/Using_Disco_Diffusion:_All_Settings#Advanced_Settings_-_Perlin_Noise)\n",
        "\n",
        "The\"Hard\" P.A.D model's settings are similar, but not quite the same. They differ as follows:\n",
        "\n",
        "- **Clip_model Selection:** ViTb32+ViTb16+ViTl14+ RN101+ RN50   | [More about steps on the Ez Charts wiki](https://ezcharts.miraheze.org/wiki/Category:CLIP_Model_selectors)\n",
        "\n",
        "- **steps:** 75 (vs 100)| [More about Clip_model selectors on the Ez Charts wiki](https://ezcharts.miraheze.org/wiki/Category:Steps)\n",
        "\n",
        "- **skip_steps:** 7 (vs 10) | [More about skip_steps on the Ez Charts wiki](https://ezcharts.miraheze.org/wiki/Category:Skip_steps)\n",
        "\n",
        "- **eta:** .75 (vs 1) | [More about eta on the Ez Charts wiki](https://ezcharts.miraheze.org/wiki/Category:Eta)\n",
        "\n",
        "\n",
        "**Model 3: \"P.A.D 4k\"**\n",
        "Default PAD model as of V2. Trained on over 4k hand-picked pixel art images. \n",
        "The notebook's default settings are just *loose* guidelines that I’ve found work well; they are NOT hard and fast rules. Based on your prompts, prompt structure, and preferences, the settings that work best for you may be quite different. Experiment, don’t get discouraged and, above all, HAVE FUN. If you want to look at more in-depth information regarding any setting or a number of setting combinations, have a look at the [EZ Charts Wiki](https://ezcharts.miraheze.org/wiki/Main_Page).\n",
        "\n",
        "-------\n",
        "## Model Output Examples\n",
        "\n",
        "As you can see below, the \"best\" model and setting for one prompt is by no means always the best for another prompt (higher quality image [here](https://i.imgur.com/rUF13B3.png)) for V.1 and [here](https://i.imgur.com/lAb2HU1.jpg) for V.2)\n",
        "\n",
        "#### V.1 Models and Settings\n",
        "\n",
        "\n",
        "<img src=\"https://i.imgur.com/rUF13B3l.png\"\n",
        "     alt=\"Pixel Art Diffusion Model Comparisons V.1\"\n",
        "     style=\"float: left; margin-right: 10px;\" />\n",
        "\n",
        "\n",
        "#### V.2 Models and Settings\n",
        "\n",
        "\n",
        "<img src=\"https://i.imgur.com/lAb2HU1.jpg\"\n",
        "     alt=\"Pixel Art Diffusion Model Comparisons V.2\"\n",
        "     style=\"float: left; margin-right: 10px;\" />\n",
        "\n",
        "\n",
        "###Additional Image Processing\n",
        "\n",
        "At this time, Pixel Art Diffusion does a pretty good job of capturing the pixel art aesthetic, but it is not pixel-perfect. If you want to create an image that *is* pixel-perfect, you can always give your PAD outputs a pass through pixellation tools such as [Pixelator](http://pixelatorapp.com/) or Pixlr’s “pixellate” filter tool. \n",
        "Once the image is pixel-perfect, you can upscale it to your desired size using [Lospec’s Pixel Art Scaler](https://lospec.com/pixel-art-scaler/).\n",
        "\n",
        "###Experimental\n",
        "Try passing your outputs back through PAD an an init image (for best results, you should probably use the same seed you used for the init). This *might* be a disaster, but it might also improve overall pixellated/aesthetic effect and details. \n",
        "\n",
        "----\n",
        "## Pixel Art Diffusion Changelog:\n",
        "\n",
        "\n",
        "06.1.2022:\n",
        "- **V1.0** Release\n",
        "- **V1.001** T4 and V100 fix by [Multimodal Ai Art](https://twitter.com/multimodalart)\n",
        "- **V1.002** The previous fix doesn't work on all occasions, so we let the users know if they have a borked GPU\n",
        "06.06.2022\n",
        "- **V2.0** Relese; \n",
        " - Added [Zippy](https://twitter.com/zippy731)'s [ColorFX Kit](https://colab.research.google.com/github/zippy731/disco-diffusion-turbo/blob/skunk/Disco_Diffusion.ipynb) to Turbo settings\n",
        " - Added support for secondary model (not default because I don't like the effect much for PAD)\n",
        " - Added a new, 3x bigger default model and updated all default settings acordingly.\n",
        "   - Runs now take around 10 s/it on a P100\n",
        "   - Defaults can now run on a T4\n",
        "- **V2.01** \n",
        " - Changed default **ic_cut_pow** from 50000 to 1000; this should resolve some issues with detail and oversaturation for many prompts\n",
        " - Removed GPU warning, as it is out of date given current defaults\n",
        " - Removed duplicate \"Check GPU Status\" cell\n",
        " - Fixed error where \"Hard\" and \"Soft\" models would not load\n",
        " - Resolved issue with log saving (I hope!)\n",
        "-----\n",
        "That's it for now!! If you enjoy the notebook, feel free to follow me on [twitter](https://twitter.com/kaliyuga_ai) and/or buy me a coffee on [patreon](https://www.patreon.com/kaliyuga_ai).\n",
        "\n",
        "Go forth and art!\n",
        "\n",
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TutorialTop"
      },
      "source": [
        "# Tutorial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiffusionSet"
      },
      "source": [
        "**Diffusion settings (Defaults are heavily outdated)**\n",
        "---\n",
        "Disco Diffusion is complex, and continually evolving with new features.  The most current documentation on on Disco Diffusion settings can be found in the unofficial guidebook:\n",
        "\n",
        "[Zippy's Disco Diffusion Cheatsheet](https://docs.google.com/document/d/1l8s7uS2dGqjztYSjPpzlmXLjl5PM3IGkRWI3IiCuK7g/edit)\n",
        "\n",
        "We also encourage users to join the [Disco Diffusion User Discord](https://discord.gg/XGZrFFCRfN) to learn from the active user community.\n",
        "\n",
        "This section below is outdated as of v2\n",
        "\n",
        "Setting | Description | Default\n",
        "--- | --- | ---\n",
        "**Your vision:**\n",
        "`text_prompts` | A description of what you'd like the machine to generate. Think of it like writing the caption below your image on a website. | N/A\n",
        "`image_prompts` | Think of these images more as a description of their contents. | N/A\n",
        "**Image quality:**\n",
        "`clip_guidance_scale`  | Controls how much the image should look like the prompt. | 1000\n",
        "`tv_scale` | Controls the smoothness of the final output. | 150\n",
        "`range_scale` | Controls how far out of range RGB values are allowed to be. | 150\n",
        "`sat_scale` | Controls how much saturation is allowed. From nshepperd's JAX notebook. | 0\n",
        "`cutn` | Controls how many crops to take from the image. | 16\n",
        "`cutn_batches` | Accumulate CLIP gradient from multiple batches of cuts. | 2\n",
        "**Init settings:**\n",
        "`init_image` | URL or local path | None\n",
        "`init_scale` | This enhances the effect of the init image, a good value is 1000 | 0\n",
        "`skip_steps` | Controls the starting point along the diffusion timesteps | 0\n",
        "`perlin_init` | Option to start with random perlin noise | False\n",
        "`perlin_mode` | ('gray', 'color') | 'mixed'\n",
        "**Advanced:**\n",
        "`skip_augs` | Controls whether to skip torchvision augmentations | False\n",
        "`randomize_class` | Controls whether the imagenet class is randomly changed each iteration | True\n",
        "`clip_denoised` | Determines whether CLIP discriminates a noisy or denoised image | False\n",
        "`clamp_grad` | Experimental: Using adaptive clip grad in the cond_fn | True\n",
        "`seed`  | Choose a random seed and print it at end of run for reproduction | random_seed\n",
        "`fuzzy_prompt` | Controls whether to add multiple noisy prompts to the prompt losses | False\n",
        "`rand_mag` | Controls the magnitude of the random noise | 0.1\n",
        "`eta` | DDIM hyperparameter | 0.5\n",
        "\n",
        "..\n",
        "\n",
        "**Model settings**\n",
        "---\n",
        "\n",
        "Setting | Description | Default\n",
        "--- | --- | ---\n",
        "**Diffusion:**\n",
        "`timestep_respacing` | Modify this value to decrease the number of timesteps. | ddim100\n",
        "`diffusion_steps` || 1000\n",
        "**Diffusion:**\n",
        "`clip_models` | Models of CLIP to load. Typically the more, the better but they all come at a hefty VRAM cost. | ViT-B/32, ViT-B/16, RN50x4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SetupTop"
      },
      "source": [
        "# 1. Set Up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 Prepare Folders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "PrepFolders"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Google Colab not detected.\n"
          ]
        }
      ],
      "source": [
        "import subprocess, os, sys, ipykernel\n",
        "\n",
        "def gitclone(url):\n",
        "    res = subprocess.run(['git', 'clone', url], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    print(res)\n",
        "\n",
        "def pipi(modulestr):\n",
        "    res = subprocess.run(['pip', 'install', modulestr], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    print(res)\n",
        "\n",
        "def pipie(modulestr):\n",
        "    res = subprocess.run(['git', 'install', '-e', modulestr], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    print(res)\n",
        "\n",
        "def wget(url, outputdir):\n",
        "    res = subprocess.run(['wget', url, '-P', f'{outputdir}'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    print(res)\n",
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    print(\"Google Colab detected. Using Google Drive.\")\n",
        "    is_colab = True\n",
        "    #@markdown If you connect your Google Drive, you can save the final image of each run on your drive.\n",
        "    google_drive = True #@param {type:\"boolean\"}\n",
        "    #@markdown Click here if you'd like to save the diffusion model checkpoint file to (and/or load from) your Google Drive:\n",
        "    save_models_to_google_drive = True #@param {type:\"boolean\"}\n",
        "except:\n",
        "    is_colab = False\n",
        "    google_drive = False\n",
        "    save_models_to_google_drive = False\n",
        "    print(\"Google Colab not detected.\")\n",
        "\n",
        "if is_colab:\n",
        "    if google_drive is True:\n",
        "        drive.mount('/content/drive')\n",
        "        root_path = '/content/drive/MyDrive/AI/Disco_Diffusion'\n",
        "    else:\n",
        "        root_path = '/content'\n",
        "else:\n",
        "    root_path = os.getcwd()\n",
        "\n",
        "import os\n",
        "def createPath(filepath):\n",
        "    os.makedirs(filepath, exist_ok=True)\n",
        "\n",
        "initDirPath = f'{root_path}/init_images'\n",
        "createPath(initDirPath)\n",
        "outDirPath = f'{root_path}/images_out'\n",
        "createPath(outDirPath)\n",
        "\n",
        "if is_colab:\n",
        "    if google_drive and not save_models_to_google_drive or not google_drive:\n",
        "        model_path = '/content/models'\n",
        "        createPath(model_path)\n",
        "    if google_drive and save_models_to_google_drive:\n",
        "        model_path = f'{root_path}/models'\n",
        "        createPath(model_path)\n",
        "else:\n",
        "    model_path = f'{root_path}/models'\n",
        "    createPath(model_path)\n",
        "\n",
        "# libraries = f'{root_path}/libraries'\n",
        "# createPath(libraries)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.3 Install, import dependencies and set up runtime devices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "id": "InstallDeps"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "disco_xform_utils.py failed to import InferenceHelper. Please ensure that AdaBins directory is in the path (i.e. via sys.path.append('./AdaBins') or other means).\n",
            "Using device: cuda:0\n"
          ]
        }
      ],
      "source": [
        "nvidiasmi_output = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "cards_requiring_downgrade = [\"Tesla T4\", \"V100\"]\n",
        "if any(cardstr in nvidiasmi_output for cardstr in cards_requiring_downgrade):\n",
        "    downgrade_pytorch_result = subprocess.run(['pip', 'install', 'torch==1.10.2', 'torchvision==0.11.3', '-q'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "\n",
        "import pathlib, shutil, os, sys\n",
        "\n",
        "if not is_colab:\n",
        "  # If running locally, there's a good chance your env will need this in order to not crash upon np.matmul() or similar operations.\n",
        "  os.environ['KMP_DUPLICATE_LIB_OK']='TRUE'\n",
        "\n",
        "\n",
        "PROJECT_DIR = os.path.abspath(os.getcwd())\n",
        "USE_ADABINS = True\n",
        "\n",
        "if is_colab:\n",
        "    if not google_drive:\n",
        "        root_path = f'/content'\n",
        "        model_path = '/content/models' \n",
        "else:\n",
        "    root_path = os.getcwd()\n",
        "    model_path = f'{root_path}/models'\n",
        "\n",
        "if is_colab:\n",
        "    subprocess.run(['apt', 'install', 'imagemagick'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "\n",
        "try:\n",
        "    from CLIP import clip\n",
        "except:\n",
        "    if not os.path.exists(\"CLIP\"):\n",
        "        gitclone(\"https://github.com/openai/CLIP\")\n",
        "    sys.path.append(f'{PROJECT_DIR}/CLIP')\n",
        "\n",
        "try:\n",
        "    from guided_diffusion.script_util import create_model_and_diffusion\n",
        "except:\n",
        "    if not os.path.exists(\"guided-diffusion\"):\n",
        "        gitclone(\"https://github.com/crowsonkb/guided-diffusion\")\n",
        "    sys.path.append(f'{PROJECT_DIR}/guided-diffusion')\n",
        "\n",
        "try:\n",
        "    from resize_right import resize\n",
        "except:\n",
        "    if not os.path.exists(\"ResizeRight\"):\n",
        "        gitclone(\"https://github.com/assafshocher/ResizeRight.git\")\n",
        "    sys.path.append(f'{PROJECT_DIR}/ResizeRight')\n",
        "\n",
        "try:\n",
        "    import py3d_tools\n",
        "except:\n",
        "    if not os.path.exists('pytorch3d-lite'):\n",
        "        gitclone(\"https://github.com/MSFTserver/pytorch3d-lite.git\")\n",
        "    sys.path.append(f'{PROJECT_DIR}/pytorch3d-lite')\n",
        "\n",
        "try:\n",
        "    from midas.dpt_depth import DPTDepthModel\n",
        "except:\n",
        "    if not os.path.exists('MiDaS'):\n",
        "        gitclone(\"https://github.com/isl-org/MiDaS.git\")\n",
        "    if not os.path.exists('MiDaS/midas_utils.py'):\n",
        "        shutil.move('MiDaS/utils.py', 'MiDaS/midas_utils.py')\n",
        "    if not os.path.exists(f'{model_path}/dpt_large-midas-2f21e586.pt'):\n",
        "        wget(\"https://github.com/intel-isl/DPT/releases/download/1_0/dpt_large-midas-2f21e586.pt\", model_path)\n",
        "    sys.path.append(f'{PROJECT_DIR}/MiDaS')\n",
        "\n",
        "try:\n",
        "    sys.path.append(PROJECT_DIR)\n",
        "    import disco_xform_utils as dxf\n",
        "except:\n",
        "    if not os.path.exists(\"disco-diffusion\"):\n",
        "        gitclone(\"https://github.com/alembics/disco-diffusion.git\")\n",
        "    if not os.path.exists('disco_xform_utils.py'):\n",
        "        shutil.move('disco-diffusion/disco_xform_utils.py', 'disco_xform_utils.py')\n",
        "    sys.path.append(PROJECT_DIR)\n",
        "\n",
        "import torch\n",
        "from dataclasses import dataclass\n",
        "from functools import partial\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import gc\n",
        "import io\n",
        "import math\n",
        "import timm\n",
        "from IPython import display\n",
        "import lpips\n",
        "from PIL import Image, ImageOps\n",
        "import requests\n",
        "from glob import glob\n",
        "import json\n",
        "from types import SimpleNamespace\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import torchvision.transforms as T\n",
        "import torchvision.transforms.functional as TF\n",
        "from tqdm.notebook import tqdm\n",
        "from CLIP import clip\n",
        "from resize_right import resize\n",
        "from guided_diffusion.script_util import create_model_and_diffusion, model_and_diffusion_defaults\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import StrMethodFormatter\n",
        "import random\n",
        "from ipywidgets import Output\n",
        "import hashlib\n",
        "from functools import partial\n",
        "if is_colab:\n",
        "    os.chdir('/content')\n",
        "    from google.colab import files\n",
        "else:\n",
        "    os.chdir(f'{PROJECT_DIR}')\n",
        "from IPython.display import Image as ipyimg\n",
        "from numpy import asarray\n",
        "from einops import rearrange, repeat\n",
        "import torch, torchvision\n",
        "import time\n",
        "from omegaconf import OmegaConf\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "# AdaBins stuff\n",
        "if USE_ADABINS:\n",
        "    try:\n",
        "        from infer import InferenceHelper\n",
        "    except:\n",
        "        if not os.path.exists(\"AdaBins\"):\n",
        "            gitclone(\"https://github.com/shariqfarooq123/AdaBins.git\")\n",
        "        if not os.path.exists(f'{PROJECT_DIR}/pretrained/AdaBins_nyu.pt'):\n",
        "            createPath(f'{PROJECT_DIR}/pretrained')\n",
        "            wget(\"https://cloudflare-ipfs.com/ipfs/Qmd2mMnDLWePKmgfS8m6ntAg4nhV5VkUyAydYBp8cWWeB7/AdaBins_nyu.pt\", f'{PROJECT_DIR}/pretrained')\n",
        "        sys.path.append(f'{PROJECT_DIR}/AdaBins')\n",
        "    from infer import InferenceHelper\n",
        "    MAX_ADABINS_AREA = 500000\n",
        "\n",
        "import torch\n",
        "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', DEVICE)\n",
        "device = DEVICE # At least one of the modules expects this name..\n",
        "\n",
        "if torch.cuda.get_device_capability(DEVICE) == (8,0): ## A100 fix thanks to Emad\n",
        "  print('Disabling CUDNN for A100 gpu', file=sys.stderr)\n",
        "  torch.backends.cudnn.enabled = False\n",
        "\n",
        "#zippy noise_injector stuff csa\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import argparse\n",
        "import imutils\n",
        "import cv2\n",
        "from skimage import exposure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.4 Define Midas functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "id": "DefMidasFns"
      },
      "outputs": [],
      "source": [
        "from midas.dpt_depth import DPTDepthModel\n",
        "from midas.midas_net import MidasNet\n",
        "from midas.midas_net_custom import MidasNet_small\n",
        "from midas.transforms import Resize, NormalizeImage, PrepareForNet\n",
        "\n",
        "# Initialize MiDaS depth model.\n",
        "# It remains resident in VRAM and likely takes around 2GB VRAM.\n",
        "# You could instead initialize it for each frame (and free it after each frame) to save VRAM.. but initializing it is slow.\n",
        "default_models = {\n",
        "    \"midas_v21_small\": f\"{model_path}/midas_v21_small-70d6b9c8.pt\",\n",
        "    \"midas_v21\": f\"{model_path}/midas_v21-f6b98070.pt\",\n",
        "    \"dpt_large\": f\"{model_path}/dpt_large-midas-2f21e586.pt\",\n",
        "    \"dpt_hybrid\": f\"{model_path}/dpt_hybrid-midas-501f0c75.pt\",\n",
        "    \"dpt_hybrid_nyu\": f\"{model_path}/dpt_hybrid_nyu-2ce69ec7.pt\",}\n",
        "\n",
        "\n",
        "def init_midas_depth_model(midas_model_type=\"dpt_large\", optimize=True):\n",
        "    midas_model = None\n",
        "    net_w = None\n",
        "    net_h = None\n",
        "    resize_mode = None\n",
        "    normalization = None\n",
        "\n",
        "    print(f\"Initializing MiDaS '{midas_model_type}' depth model...\")\n",
        "    # load network\n",
        "    midas_model_path = default_models[midas_model_type]\n",
        "\n",
        "    if midas_model_type == \"dpt_large\": # DPT-Large\n",
        "        midas_model = DPTDepthModel(\n",
        "            path=midas_model_path,\n",
        "            backbone=\"vitl16_384\",\n",
        "            non_negative=True,\n",
        "        )\n",
        "        net_w, net_h = 384, 384\n",
        "        resize_mode = \"minimal\"\n",
        "        normalization = NormalizeImage(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "    elif midas_model_type == \"dpt_hybrid\": #DPT-Hybrid\n",
        "        midas_model = DPTDepthModel(\n",
        "            path=midas_model_path,\n",
        "            backbone=\"vitb_rn50_384\",\n",
        "            non_negative=True,\n",
        "        )\n",
        "        net_w, net_h = 384, 384\n",
        "        resize_mode=\"minimal\"\n",
        "        normalization = NormalizeImage(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "    elif midas_model_type == \"dpt_hybrid_nyu\": #DPT-Hybrid-NYU\n",
        "        midas_model = DPTDepthModel(\n",
        "            path=midas_model_path,\n",
        "            backbone=\"vitb_rn50_384\",\n",
        "            non_negative=True,\n",
        "        )\n",
        "        net_w, net_h = 384, 384\n",
        "        resize_mode=\"minimal\"\n",
        "        normalization = NormalizeImage(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "    elif midas_model_type == \"midas_v21\":\n",
        "        midas_model = MidasNet(midas_model_path, non_negative=True)\n",
        "        net_w, net_h = 384, 384\n",
        "        resize_mode=\"upper_bound\"\n",
        "        normalization = NormalizeImage(\n",
        "            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
        "        )\n",
        "    elif midas_model_type == \"midas_v21_small\":\n",
        "        midas_model = MidasNet_small(midas_model_path, features=64, backbone=\"efficientnet_lite3\", exportable=True, non_negative=True, blocks={'expand': True})\n",
        "        net_w, net_h = 256, 256\n",
        "        resize_mode=\"upper_bound\"\n",
        "        normalization = NormalizeImage(\n",
        "            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
        "        )\n",
        "    else:\n",
        "        print(f\"midas_model_type '{midas_model_type}' not implemented\")\n",
        "        assert False\n",
        "\n",
        "    midas_transform = T.Compose(\n",
        "        [\n",
        "            Resize(\n",
        "                net_w,\n",
        "                net_h,\n",
        "                resize_target=None,\n",
        "                keep_aspect_ratio=True,\n",
        "                ensure_multiple_of=32,\n",
        "                resize_method=resize_mode,\n",
        "                image_interpolation_method=cv2.INTER_CUBIC,\n",
        "            ),\n",
        "            normalization,\n",
        "            PrepareForNet(),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    midas_model.eval()\n",
        "    \n",
        "    if optimize==True:\n",
        "        if DEVICE == torch.device(\"cuda\"):\n",
        "            midas_model = midas_model.to(memory_format=torch.channels_last)  \n",
        "            midas_model = midas_model.half()\n",
        "\n",
        "    midas_model.to(DEVICE)\n",
        "\n",
        "    print(f\"MiDaS '{midas_model_type}' depth model initialized.\")\n",
        "    return midas_model, midas_transform, net_w, net_h, resize_mode, normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.5 Define necessary functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "id": "DefFns"
      },
      "outputs": [],
      "source": [
        "# https://gist.github.com/adefossez/0646dbe9ed4005480a2407c62aac8869\n",
        "\n",
        "import py3d_tools as p3dT\n",
        "import disco_xform_utils as dxf\n",
        "\n",
        "def interp(t):\n",
        "    return 3 * t**2 - 2 * t ** 3\n",
        "\n",
        "def perlin(width, height, scale=10, device=None):\n",
        "    gx, gy = torch.randn(2, width + 1, height + 1, 1, 1, device=device)\n",
        "    xs = torch.linspace(0, 1, scale + 1)[:-1, None].to(device)\n",
        "    ys = torch.linspace(0, 1, scale + 1)[None, :-1].to(device)\n",
        "    wx = 1 - interp(xs)\n",
        "    wy = 1 - interp(ys)\n",
        "    dots = 0\n",
        "    dots += wx * wy * (gx[:-1, :-1] * xs + gy[:-1, :-1] * ys)\n",
        "    dots += (1 - wx) * wy * (-gx[1:, :-1] * (1 - xs) + gy[1:, :-1] * ys)\n",
        "    dots += wx * (1 - wy) * (gx[:-1, 1:] * xs - gy[:-1, 1:] * (1 - ys))\n",
        "    dots += (1 - wx) * (1 - wy) * (-gx[1:, 1:] * (1 - xs) - gy[1:, 1:] * (1 - ys))\n",
        "    return dots.permute(0, 2, 1, 3).contiguous().view(width * scale, height * scale)\n",
        "\n",
        "def perlin_ms(octaves, width, height, grayscale, device=device):\n",
        "    out_array = [0.5] if grayscale else [0.5, 0.5, 0.5]\n",
        "    # out_array = [0.0] if grayscale else [0.0, 0.0, 0.0]\n",
        "    for i in range(1 if grayscale else 3):\n",
        "        scale = 2 ** len(octaves)\n",
        "        oct_width = width\n",
        "        oct_height = height\n",
        "        for oct in octaves:\n",
        "            p = perlin(oct_width, oct_height, scale, device)\n",
        "            out_array[i] += p * oct\n",
        "            scale //= 2\n",
        "            oct_width *= 2\n",
        "            oct_height *= 2\n",
        "    return torch.cat(out_array)\n",
        "\n",
        "def create_perlin_noise(octaves=[1, 1, 1, 1], width=2, height=2, grayscale=True):\n",
        "    out = perlin_ms(octaves, width, height, grayscale)\n",
        "    if grayscale:\n",
        "        out = TF.resize(size=(side_y, side_x), img=out.unsqueeze(0))\n",
        "        out = TF.to_pil_image(out.clamp(0, 1)).convert('RGB')\n",
        "    else:\n",
        "        out = out.reshape(-1, 3, out.shape[0]//3, out.shape[1])\n",
        "        out = TF.resize(size=(side_y, side_x), img=out)\n",
        "        out = TF.to_pil_image(out.clamp(0, 1).squeeze())\n",
        "\n",
        "    out = ImageOps.autocontrast(out)\n",
        "    return out\n",
        "\n",
        "def regen_perlin():\n",
        "    if perlin_mode == 'color':\n",
        "        init = create_perlin_noise([1.5**-i*0.5 for i in range(12)], 1, 1, False)\n",
        "        init2 = create_perlin_noise([1.5**-i*0.5 for i in range(8)], 4, 4, False)\n",
        "    elif perlin_mode == 'gray':\n",
        "        init = create_perlin_noise([1.5**-i*0.5 for i in range(12)], 1, 1, True)\n",
        "        init2 = create_perlin_noise([1.5**-i*0.5 for i in range(8)], 4, 4, True)\n",
        "    else:\n",
        "        init = create_perlin_noise([1.5**-i*0.5 for i in range(12)], 1, 1, False)\n",
        "        init2 = create_perlin_noise([1.5**-i*0.5 for i in range(8)], 4, 4, True)\n",
        "\n",
        "    init = TF.to_tensor(init).add(TF.to_tensor(init2)).div(2).to(device).unsqueeze(0).mul(2).sub(1)\n",
        "    del init2\n",
        "    return init.expand(batch_size, -1, -1, -1)\n",
        "\n",
        "def fetch(url_or_path):\n",
        "    if str(url_or_path).startswith('http://') or str(url_or_path).startswith('https://'):\n",
        "        r = requests.get(url_or_path)\n",
        "        r.raise_for_status()\n",
        "        fd = io.BytesIO()\n",
        "        fd.write(r.content)\n",
        "        fd.seek(0)\n",
        "        return fd\n",
        "    return open(url_or_path, 'rb')\n",
        "\n",
        "def read_image_workaround(path):\n",
        "    \"\"\"OpenCV reads images as BGR, Pillow saves them as RGB. Work around\n",
        "    this incompatibility to avoid colour inversions.\"\"\"\n",
        "    im_tmp = cv2.imread(path)\n",
        "    return cv2.cvtColor(im_tmp, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "def parse_prompt(prompt):\n",
        "    if prompt.startswith('http://') or prompt.startswith('https://'):\n",
        "        vals = prompt.rsplit(':', 2)\n",
        "        vals = [vals[0] + ':' + vals[1], *vals[2:]]\n",
        "    else:\n",
        "        vals = prompt.rsplit(':', 1)\n",
        "    vals = vals + ['', '1'][len(vals):]\n",
        "    return vals[0], float(vals[1])\n",
        "\n",
        "def sinc(x):\n",
        "    return torch.where(x != 0, torch.sin(math.pi * x) / (math.pi * x), x.new_ones([]))\n",
        "\n",
        "def lanczos(x, a):\n",
        "    cond = torch.logical_and(-a < x, x < a)\n",
        "    out = torch.where(cond, sinc(x) * sinc(x/a), x.new_zeros([]))\n",
        "    return out / out.sum()\n",
        "\n",
        "def ramp(ratio, width):\n",
        "    n = math.ceil(width / ratio + 1)\n",
        "    out = torch.empty([n])\n",
        "    cur = 0\n",
        "    for i in range(out.shape[0]):\n",
        "        out[i] = cur\n",
        "        cur += ratio\n",
        "    return torch.cat([-out[1:].flip([0]), out])[1:-1]\n",
        "\n",
        "def resample(input, size, align_corners=True):\n",
        "    n, c, h, w = input.shape\n",
        "    dh, dw = size\n",
        "\n",
        "    input = input.reshape([n * c, 1, h, w])\n",
        "\n",
        "    if dh < h:\n",
        "        kernel_h = lanczos(ramp(dh / h, 2), 2).to(input.device, input.dtype)\n",
        "        pad_h = (kernel_h.shape[0] - 1) // 2\n",
        "        input = F.pad(input, (0, 0, pad_h, pad_h), 'reflect')\n",
        "        input = F.conv2d(input, kernel_h[None, None, :, None])\n",
        "\n",
        "    if dw < w:\n",
        "        kernel_w = lanczos(ramp(dw / w, 2), 2).to(input.device, input.dtype)\n",
        "        pad_w = (kernel_w.shape[0] - 1) // 2\n",
        "        input = F.pad(input, (pad_w, pad_w, 0, 0), 'reflect')\n",
        "        input = F.conv2d(input, kernel_w[None, None, None, :])\n",
        "\n",
        "    input = input.reshape([n, c, h, w])\n",
        "    return F.interpolate(input, size, mode='bicubic', align_corners=align_corners)\n",
        "\n",
        "class MakeCutouts(nn.Module):\n",
        "    def __init__(self, cut_size, cutn, skip_augs=False):\n",
        "        super().__init__()\n",
        "        self.cut_size = cut_size\n",
        "        self.cutn = cutn\n",
        "        self.skip_augs = skip_augs\n",
        "        self.augs = T.Compose([\n",
        "            T.RandomHorizontalFlip(p=0.5),\n",
        "            T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
        "            T.RandomAffine(degrees=15, translate=(0.1, 0.1)),\n",
        "            T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
        "            T.RandomPerspective(distortion_scale=0.4, p=0.7),\n",
        "            T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
        "            T.RandomGrayscale(p=0.15),\n",
        "            T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
        "            # T.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
        "        ])\n",
        "\n",
        "    def forward(self, input):\n",
        "        input = T.Pad(input.shape[2]//4, fill=0)(input)\n",
        "        sideY, sideX = input.shape[2:4]\n",
        "        max_size = min(sideX, sideY)\n",
        "\n",
        "        cutouts = []\n",
        "        for ch in range(self.cutn):\n",
        "            if ch > self.cutn - self.cutn//4:\n",
        "                cutout = input.clone()\n",
        "            else:\n",
        "                size = int(max_size * torch.zeros(1,).normal_(mean=.8, std=.3).clip(float(self.cut_size/max_size), 1.))\n",
        "                offsetx = torch.randint(0, abs(sideX - size + 1), ())\n",
        "                offsety = torch.randint(0, abs(sideY - size + 1), ())\n",
        "                cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
        "\n",
        "            if not self.skip_augs:\n",
        "                cutout = self.augs(cutout)\n",
        "            cutouts.append(resample(cutout, (self.cut_size, self.cut_size)))\n",
        "            del cutout\n",
        "\n",
        "        cutouts = torch.cat(cutouts, dim=0)\n",
        "        return cutouts\n",
        "\n",
        "cutout_debug = False\n",
        "padargs = {}\n",
        "\n",
        "class MakeCutoutsDango(nn.Module):\n",
        "    def __init__(self, cut_size,\n",
        "                 Overview=4, \n",
        "                 InnerCrop = 0, IC_Size_Pow=0.5, IC_Grey_P = 0.2\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        self.cut_size = cut_size\n",
        "        self.Overview = Overview\n",
        "        self.InnerCrop = InnerCrop\n",
        "        self.IC_Size_Pow = IC_Size_Pow\n",
        "        self.IC_Grey_P = IC_Grey_P\n",
        "        if args.animation_mode == 'None':\n",
        "          self.augs = T.Compose([\n",
        "              T.RandomHorizontalFlip(p=0.5),\n",
        "              T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
        "              T.RandomAffine(degrees=10, translate=(0.05, 0.05),  interpolation = T.InterpolationMode.BILINEAR),\n",
        "              T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
        "              T.RandomGrayscale(p=0.1),\n",
        "              T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
        "              T.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
        "          ])\n",
        "        elif args.animation_mode == 'Video Input':\n",
        "          self.augs = T.Compose([\n",
        "              T.RandomHorizontalFlip(p=0.5),\n",
        "              T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
        "              T.RandomAffine(degrees=15, translate=(0.1, 0.1)),\n",
        "              T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
        "              T.RandomPerspective(distortion_scale=0.4, p=0.7),\n",
        "              T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
        "              T.RandomGrayscale(p=0.15),\n",
        "              T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
        "              # T.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
        "          ])\n",
        "        elif  args.animation_mode == '2D' or args.animation_mode == '3D':\n",
        "          self.augs = T.Compose([\n",
        "              T.RandomHorizontalFlip(p=0.4),\n",
        "              T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
        "              T.RandomAffine(degrees=10, translate=(0.05, 0.05),  interpolation = T.InterpolationMode.BILINEAR),\n",
        "              T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
        "              T.RandomGrayscale(p=0.1),\n",
        "              T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
        "              T.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.3),\n",
        "          ])\n",
        "          \n",
        "\n",
        "    def forward(self, input):\n",
        "        cutouts = []\n",
        "        gray = T.Grayscale(3)\n",
        "        sideY, sideX = input.shape[2:4]\n",
        "        max_size = min(sideX, sideY)\n",
        "        min_size = min(sideX, sideY, self.cut_size)\n",
        "        l_size = max(sideX, sideY)\n",
        "        output_shape = [1,3,self.cut_size,self.cut_size] \n",
        "        output_shape_2 = [1,3,self.cut_size+2,self.cut_size+2]\n",
        "        pad_input = F.pad(input,((sideY-max_size)//2,(sideY-max_size)//2,(sideX-max_size)//2,(sideX-max_size)//2), **padargs)\n",
        "        cutout = resize(pad_input, out_shape=output_shape)\n",
        "\n",
        "        if self.Overview>0:\n",
        "            if self.Overview<=4:\n",
        "                if self.Overview>=1:\n",
        "                    cutouts.append(cutout)\n",
        "                if self.Overview>=2:\n",
        "                    cutouts.append(gray(cutout))\n",
        "                if self.Overview>=3:\n",
        "                    cutouts.append(TF.hflip(cutout))\n",
        "                if self.Overview==4:\n",
        "                    cutouts.append(gray(TF.hflip(cutout)))\n",
        "            else:\n",
        "                cutout = resize(pad_input, out_shape=output_shape)\n",
        "                for _ in range(self.Overview):\n",
        "                    cutouts.append(cutout)\n",
        "\n",
        "            if cutout_debug:\n",
        "                if is_colab:\n",
        "                    TF.to_pil_image(cutouts[0].clamp(0, 1).squeeze(0)).save(\"/content/cutout_overview0.jpg\",quality=99)\n",
        "                else:\n",
        "                    TF.to_pil_image(cutouts[0].clamp(0, 1).squeeze(0)).save(\"cutout_overview0.jpg\",quality=99)\n",
        "\n",
        "                              \n",
        "        if self.InnerCrop >0:\n",
        "            for i in range(self.InnerCrop):\n",
        "                size = int(torch.rand([])**self.IC_Size_Pow * (max_size - min_size) + min_size)\n",
        "                offsetx = torch.randint(0, sideX - size + 1, ())\n",
        "                offsety = torch.randint(0, sideY - size + 1, ())\n",
        "                cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
        "                if i <= int(self.IC_Grey_P * self.InnerCrop):\n",
        "                    cutout = gray(cutout)\n",
        "                cutout = resize(cutout, out_shape=output_shape)\n",
        "                cutouts.append(cutout)\n",
        "            if cutout_debug:\n",
        "                if is_colab:\n",
        "                    TF.to_pil_image(cutouts[-1].clamp(0, 1).squeeze(0)).save(\"/content/cutout_InnerCrop.jpg\",quality=99)\n",
        "                else:\n",
        "                    TF.to_pil_image(cutouts[-1].clamp(0, 1).squeeze(0)).save(\"cutout_InnerCrop.jpg\",quality=99)\n",
        "        cutouts = torch.cat(cutouts)\n",
        "        if skip_augs is not True: cutouts=self.augs(cutouts)\n",
        "        return cutouts\n",
        "\n",
        "def spherical_dist_loss(x, y):\n",
        "    x = F.normalize(x, dim=-1)\n",
        "    y = F.normalize(y, dim=-1)\n",
        "    return (x - y).norm(dim=-1).div(2).arcsin().pow(2).mul(2)     \n",
        "\n",
        "def tv_loss(input):\n",
        "    \"\"\"L2 total variation loss, as in Mahendran et al.\"\"\"\n",
        "    input = F.pad(input, (0, 1, 0, 1), 'replicate')\n",
        "    x_diff = input[..., :-1, 1:] - input[..., :-1, :-1]\n",
        "    y_diff = input[..., 1:, :-1] - input[..., :-1, :-1]\n",
        "    return (x_diff**2 + y_diff**2).mean([1, 2, 3])\n",
        "\n",
        "\n",
        "def range_loss(input):\n",
        "    return (input - input.clamp(-1, 1)).pow(2).mean([1, 2, 3])\n",
        "\n",
        "stop_on_next_loop = False  # Make sure GPU memory doesn't get corrupted from cancelling the run mid-way through, allow a full frame to complete\n",
        "TRANSLATION_SCALE = 1.0/200.0\n",
        "\n",
        "def do_3d_step(img_filepath, frame_num, midas_model, midas_transform):\n",
        "  if args.key_frames:\n",
        "    translation_x = args.translation_x_series[frame_num]\n",
        "    translation_y = args.translation_y_series[frame_num]\n",
        "    translation_z = args.translation_z_series[frame_num]\n",
        "    rotation_3d_x = args.rotation_3d_x_series[frame_num]\n",
        "    rotation_3d_y = args.rotation_3d_y_series[frame_num]\n",
        "    rotation_3d_z = args.rotation_3d_z_series[frame_num]\n",
        "    print(\n",
        "        f'translation_x: {translation_x}',\n",
        "        f'translation_y: {translation_y}',\n",
        "        f'translation_z: {translation_z}',\n",
        "        f'rotation_3d_x: {rotation_3d_x}',\n",
        "        f'rotation_3d_y: {rotation_3d_y}',\n",
        "        f'rotation_3d_z: {rotation_3d_z}',\n",
        "    )\n",
        "\n",
        "  translate_xyz = [-translation_x*TRANSLATION_SCALE, translation_y*TRANSLATION_SCALE, -translation_z*TRANSLATION_SCALE]\n",
        "  rotate_xyz_degrees = [rotation_3d_x, rotation_3d_y, rotation_3d_z]\n",
        "  print('translation:',translate_xyz)\n",
        "  print('rotation:',rotate_xyz_degrees)\n",
        "  rotate_xyz = [math.radians(rotate_xyz_degrees[0]), math.radians(rotate_xyz_degrees[1]), math.radians(rotate_xyz_degrees[2])]\n",
        "  rot_mat = p3dT.euler_angles_to_matrix(torch.tensor(rotate_xyz, device=device), \"XYZ\").unsqueeze(0)\n",
        "  print(\"rot_mat: \" + str(rot_mat))\n",
        "  next_step_pil = dxf.transform_image_3d(img_filepath, midas_model, midas_transform, DEVICE,\n",
        "                                          rot_mat, translate_xyz, args.near_plane, args.far_plane,\n",
        "                                          args.fov, padding_mode=args.padding_mode,\n",
        "                                          sampling_mode=args.sampling_mode, midas_weight=args.midas_weight)\n",
        "  return next_step_pil\n",
        "\n",
        "\n",
        "def soft_limit(x):\n",
        "  #zippy soft limiter\n",
        "  #value from -n to n, always return a value -1<x<1\n",
        "  #soft_limiter_knee set in params = 0.97 # where does compression start?\n",
        "  soft_sign = x/abs(x)\n",
        "  soft_overage = ((abs(x)-soft_limiter_knee)+(abs(abs(x)-soft_limiter_knee)))/2\n",
        "  soft_base = abs(x)-soft_overage\n",
        "  soft_limited_x = soft_base + torch.tanh(soft_overage/(1-soft_limiter_knee))*(1-soft_limiter_knee)\n",
        "  return soft_limited_x*soft_sign\n",
        "\n",
        "def center_crop(img,crop_pct):\n",
        "  #zippy color kit\n",
        "  dimensions = img.shape \n",
        "  # height, width, number of channels in image\n",
        "  height = img.shape[0]\n",
        "  width = img.shape[1]\n",
        "  channels = img.shape[2]\n",
        "  h1=int((height/2)*(1-crop_pct))\n",
        "  h2=int((height/2)*(1+crop_pct))\n",
        "  w1=int((width/2)*(1-crop_pct))\n",
        "  w2=int((width/2)*(1+crop_pct))\n",
        "  crop_img = img[h1:h2, w1:w2]\n",
        "  return crop_img\n",
        "\n",
        "def get_edge_img(img):\n",
        "  #zippy color kit\n",
        "  canny_min = .66*img.mean()\n",
        "  canny_max = 1.33*img.mean()  \n",
        "  get_edge_img = cv2.Canny(img,canny_min,canny_max,True) \n",
        "  return get_edge_img\n",
        "\n",
        "def get_edge_mix(img):\n",
        "  #zippy color kit\n",
        "  edges = get_edge_img(img)\n",
        "  edge_mix = edges.mean()\n",
        "  return edge_mix\n",
        "\n",
        "def calc_contrast(img):\n",
        "  #from https://stackoverflow.com/questions/58821130/how-to-calculate-the-contrast-of-an-image\n",
        "  img_grey = cv2.cvtColor((img*255).astype(np.uint8), cv2.COLOR_BGR2GRAY)\n",
        "  contrast = img_grey.std()\n",
        "  return contrast\n",
        "\n",
        "def calc_brightness(img):\n",
        "  #zippy color kit\n",
        "  #from https://stackoverflow.com/questions/58821130/how-to-calculate-the-contrast-of-an-image\n",
        "  img_grey = cv2.cvtColor((img*255).astype(np.uint8), cv2.COLOR_BGR2GRAY)\n",
        "  brightness = img_grey.mean()\n",
        "  return brightness\n",
        "\n",
        "def contrast_stretch(img):\n",
        "  #zippy color kit\n",
        "  #requires a cv2 Image (val 0-1) \n",
        "  #also https://stackoverflow.com/questions/64484423/cv2-lut-throws-an-error-when-performing-gamma-correction-assertion-failed\n",
        "  xp = [0, 64, 128, 192, 255]\n",
        "  fp = [0, 16, 128, 240, 255]\n",
        "  x = np.arange(256)\n",
        "  table = np.interp(x, xp, fp).astype('uint8')\n",
        "  #img = cv2.LUT((255*img).astype(np.uint8), table)\n",
        "  #img = (img.astype(np.float)) / 255\n",
        "  img = cv2.LUT((img).astype(np.uint8), table)\n",
        "  img = (img.astype(np.float))\n",
        "  return img\n",
        "\n",
        "def contrast_stretch_lite(img):\n",
        "  #zippy color kit\n",
        "  xp = [0, 64, 128, 192, 255]\n",
        "  fp = [0, 32, 128, 224, 255]\n",
        "  x = np.arange(256)\n",
        "  table = np.interp(x, xp, fp).astype('uint8')\n",
        "  #img = cv2.LUT((255*img).astype(np.uint8), table)\n",
        "  #img = (img.astype(np.float)) / 255\n",
        "  img = cv2.LUT((img).astype(np.uint8), table)\n",
        "  img = (img.astype(np.float))\n",
        "  return img\n",
        "\n",
        "def add_noise(img,noise_mix,noise_injector_px_size,\n",
        "              noise_injector_spice,noise_injector_mask_feather,\n",
        "              noise_injector_sigma):\n",
        "  # zippy noise injector csa\n",
        "  # make edge map from edge image\n",
        "  width = int(img.shape[1])\n",
        "  height = int(img.shape[0])\n",
        "  dim_hw = (height, width)\n",
        "  dim_wh = (width, height)\n",
        "\n",
        "  dim = (int(img.shape[1]), int(img.shape[0]))\n",
        "  edge_img = get_edge_img(img)\n",
        "  kernel = np.ones((7,7), np.uint8)\n",
        "  kernel2 = np.ones((21,21), np.uint8)\n",
        "  edge_img_dilation = cv2.dilate(edge_img, kernel, iterations=3)\n",
        "  edge_img_dilation2 = cv2.dilate(edge_img, kernel2, iterations=3)\n",
        "  blended_dilation = (edge_img_dilation/2 + edge_img_dilation2/2)\n",
        "  blur_size = noise_injector_mask_feather*2+1\n",
        "  \n",
        "  blur_kernel = (blur_size,blur_size)\n",
        "  #3 blurs, just for grins\n",
        "  edge_blurred_img = cv2.GaussianBlur(blended_dilation, blur_kernel, 0)\n",
        "  edge_blurred_img = cv2.GaussianBlur(edge_blurred_img, blur_kernel, 0)\n",
        "  edge_blurred_img = cv2.GaussianBlur(edge_blurred_img, blur_kernel, 0)\n",
        "  mask = (255-edge_blurred_img)\n",
        "  mask = mask.reshape(*mask.shape, 1)/255\n",
        "\n",
        "  #make gaussian mask\n",
        "  #https://stackoverflow.com/questions/55261087/adding-gaussian-noise-to-image\n",
        "  mean = 0\n",
        "  #var = 100\n",
        "  #sigma = var ** 0.5\n",
        "  sigma = noise_injector_sigma\n",
        "  #sigma = 64 #larger numbers = bigger blobs\n",
        "  #noise image is expecting dim_hw\n",
        "  gaussian = np.random.normal(mean, sigma, dim_hw) # dim must be (H,W) \n",
        "  gaus_blur_kernel = (15,15)\n",
        "  blurred_gaussian = cv2.GaussianBlur(gaussian, gaus_blur_kernel, 0)\n",
        "  gaussian_mask = blurred_gaussian.reshape(*blurred_gaussian.shape, 1)\n",
        "  mask = np.clip(mask, 0, 1)\n",
        "  gaussian_mask = np.clip(gaussian_mask, 0, 1)\n",
        "  combo_mask = mask*gaussian_mask\n",
        "\n",
        "  #now make a noise pattern\n",
        "  noise_img =  np.random.normal(loc=0, scale=1, size=img.shape)\n",
        "  noise_img *= 255\n",
        "  #change pixel density...\n",
        "  noise_px_size = noise_injector_px_size  \n",
        "  noise_img = center_crop(noise_img,1/noise_px_size)\n",
        "  #modify noise color mix to original img\n",
        "  colored_noise = exposure.match_histograms(noise_img, img, multichannel=True)\n",
        "  #blend in 'spice' \n",
        "  colored_noise = noise_img * noise_injector_spice + colored_noise * (1-noise_injector_spice)  \n",
        "  \n",
        "  #resize to orig.. using dim from above\n",
        "  #cv2.resize is expecting dim_wh\n",
        "  colored_noise = cv2.resize(colored_noise, dim_wh, interpolation = cv2.INTER_AREA) \n",
        "  noise_blur_size = noise_px_size * 2 + 1\n",
        "  noise_blur_kernel = (noise_blur_size,noise_blur_size)    \n",
        "  colored_noise = cv2.GaussianBlur(colored_noise, noise_blur_kernel, 0)\n",
        "  colored_noise = cv2.GaussianBlur(colored_noise, noise_blur_kernel, 0) \n",
        "  \n",
        "  masked_noise = colored_noise * combo_mask\n",
        "  scaled_mask = combo_mask * noise_mix\n",
        "  #blend in noise using mask\n",
        "  final_img = colored_noise * scaled_mask + img * (1 - scaled_mask)\n",
        "\n",
        "  #uncomment below to see elements of noise injector\n",
        "  #cv2_imshow(edge_img)\n",
        "  #cv2_imshow(blended_dilation)\n",
        "  #cv2_imshow(mask*255)#mask is 0-1\n",
        "  #cv2_imshow(gaussian_mask*255)\n",
        "  #cv2_imshow(combo_mask*255)\n",
        "  #cv2_imshow(noise_img)\n",
        "  #cv2_imshow(colored_noise)\n",
        "  #cv2_imshow(final_img)\n",
        "  #print('corrected contrast is ',(calc_contrast(final_img)))\n",
        "  #print(calc_contrast(final_img))\n",
        "  return final_img\n",
        "\n",
        "def do_run():\n",
        "  seed = args.seed\n",
        "  print(range(args.start_frame, args.max_frames))\n",
        "\n",
        "  if (args.animation_mode == \"3D\") and (args.midas_weight > 0.0):\n",
        "      midas_model, midas_transform, midas_net_w, midas_net_h, midas_resize_mode, midas_normalization = init_midas_depth_model(args.midas_depth_model)\n",
        "  for frame_num in range(args.start_frame, args.max_frames):\n",
        "      if stop_on_next_loop:\n",
        "        break\n",
        "      \n",
        "      display.clear_output(wait=True)\n",
        "\n",
        "      # Print Frame progress if animation mode is on\n",
        "      if args.animation_mode != \"None\":\n",
        "        batchBar = tqdm(range(args.max_frames), desc =\"Frames\")\n",
        "        batchBar.n = frame_num\n",
        "        batchBar.refresh()\n",
        "\n",
        "      \n",
        "      # Inits if not video frames\n",
        "      if args.animation_mode != \"Video Input\":\n",
        "        if args.init_image in ['','none', 'None', 'NONE']:\n",
        "          init_image = None\n",
        "        else:\n",
        "          init_image = args.init_image\n",
        "        init_scale = args.init_scale\n",
        "        skip_steps = args.skip_steps\n",
        "        skip_end_steps = args.skip_end_steps\n",
        "\n",
        "      if args.animation_mode == \"2D\":\n",
        "        if args.key_frames:\n",
        "          angle = args.angle_series[frame_num]\n",
        "          zoom = args.zoom_series[frame_num]\n",
        "          translation_x = args.translation_x_series[frame_num]\n",
        "          translation_y = args.translation_y_series[frame_num]\n",
        "          print(\n",
        "              f'angle: {angle}',\n",
        "              f'zoom: {zoom}',\n",
        "              f'translation_x: {translation_x}',\n",
        "              f'translation_y: {translation_y}',\n",
        "          )\n",
        "        \n",
        "        if frame_num > 0:\n",
        "          seed += 1\n",
        "          if resume_run and frame_num == start_frame:\n",
        "            img_0 = cv2.imread(batchFolder+f\"/{batch_name}({batchNum})_{start_frame-1:04}.png\")\n",
        "          else:\n",
        "            img_0 = cv2.imread('prevFrame.png')\n",
        "          center = (1*img_0.shape[1]//2, 1*img_0.shape[0]//2)\n",
        "          trans_mat = np.float32(\n",
        "              [[1, 0, translation_x],\n",
        "              [0, 1, translation_y]]\n",
        "          )\n",
        "          rot_mat = cv2.getRotationMatrix2D( center, angle, zoom )\n",
        "          trans_mat = np.vstack([trans_mat, [0,0,1]])\n",
        "          rot_mat = np.vstack([rot_mat, [0,0,1]])\n",
        "          transformation_matrix = np.matmul(rot_mat, trans_mat)\n",
        "          img_0 = cv2.warpPerspective(\n",
        "              img_0,\n",
        "              transformation_matrix,\n",
        "              (img_0.shape[1], img_0.shape[0]),\n",
        "              borderMode=cv2.BORDER_WRAP\n",
        "          )\n",
        "\n",
        "          cv2.imwrite('prevFrameScaled.png', img_0)\n",
        "          init_image = 'prevFrameScaled.png'\n",
        "          init_scale = args.frames_scale\n",
        "          skip_steps = args.calc_frames_skip_steps\n",
        "          skip_end_steps = args.frames_skip_end_steps\n",
        "\n",
        "      if args.animation_mode == \"3D\":\n",
        "        if frame_num > 0:\n",
        "          seed += 1    \n",
        "          if resume_run and frame_num == start_frame:\n",
        "            img_filepath = batchFolder+f\"/{batch_name}({batchNum})_{start_frame-1:04}.png\"\n",
        "            if turbo_mode and frame_num > turbo_preroll:\n",
        "              shutil.copyfile(img_filepath, 'oldFrameScaled.png')\n",
        "          else:\n",
        "            img_filepath = '/content/prevFrame.png' if is_colab else 'prevFrame.png'\n",
        "\n",
        "          next_step_pil = do_3d_step(img_filepath, frame_num, midas_model, midas_transform)\n",
        "          next_step_pil.save('prevFrameScaled.png')\n",
        "\n",
        "          ### Turbo mode - skip some diffusions, use 3d morph for clarity and to save time\n",
        "          if turbo_mode:\n",
        "            if frame_num == turbo_preroll: #start tracking oldframe\n",
        "              next_step_pil.save('oldFrameScaled.png')#stash for later blending          \n",
        "            elif frame_num > turbo_preroll:\n",
        "              #set up 2 warped image sequences, old & new, to blend toward new diff image\n",
        "              old_frame = do_3d_step('oldFrameScaled.png', frame_num, midas_model, midas_transform)\n",
        "              old_frame.save('oldFrameScaled.png')\n",
        "              if frame_num % int(turbo_steps) != 0: \n",
        "                print('turbo skip this frame: skipping clip diffusion steps')\n",
        "                filename = f'{args.batch_name}({args.batchNum})_{frame_num:04}.png'\n",
        "                blend_factor = ((frame_num % int(turbo_steps))+1)/int(turbo_steps)\n",
        "                print('turbo skip this frame: skipping clip diffusion steps and saving blended frame')\n",
        "                newWarpedImg = cv2.imread('prevFrameScaled.png')#this is already updated..\n",
        "                oldWarpedImg = cv2.imread('oldFrameScaled.png')\n",
        "                blendedImage = cv2.addWeighted(newWarpedImg, blend_factor, oldWarpedImg,1-blend_factor, 0.0)\n",
        "                cv2.imwrite(f'{batchFolder}/{filename}',blendedImage)\n",
        "                next_step_pil.save(f'{img_filepath}') # save it also as prev_frame to feed next iteration\n",
        "                if vr_mode:\n",
        "                  generate_eye_views(TRANSLATION_SCALE,batchFolder,filename,frame_num,midas_model, midas_transform)\n",
        "                continue\n",
        "              else:\n",
        "                #if not a skip frame, will run diffusion and need to blend.\n",
        "                oldWarpedImg = cv2.imread('prevFrameScaled.png')\n",
        "                cv2.imwrite(f'oldFrameScaled.png',oldWarpedImg)#swap in for blending later \n",
        "                print('clip/diff this frame - generate clip diff image')\n",
        "\n",
        "          init_image = 'prevFrameScaled.png'\n",
        "          init_scale = args.frames_scale\n",
        "          skip_steps = args.calc_frames_skip_steps\n",
        "          skip_end_steps = args.frames_skip_end_steps\n",
        "\n",
        "      if  args.animation_mode == \"Video Input\":\n",
        "        if not video_init_seed_continuity:\n",
        "          seed += 1\n",
        "        init_image = f'{videoFramesFolder}/{frame_num+1:04}.jpg'\n",
        "        init_scale = args.frames_scale\n",
        "        skip_steps = args.calc_frames_skip_steps\n",
        "        skip_end_steps = args.skip_end_steps\n",
        "\n",
        "      loss_values = []\n",
        "  \n",
        "      if seed is not None:\n",
        "          np.random.seed(seed)\n",
        "          random.seed(seed)\n",
        "          torch.manual_seed(seed)\n",
        "          torch.cuda.manual_seed_all(seed)\n",
        "          torch.backends.cudnn.deterministic = True\n",
        "  \n",
        "      target_embeds, weights = [], []\n",
        "      \n",
        "      if args.prompts_series is not None and frame_num >= len(args.prompts_series):\n",
        "        frame_prompt = args.prompts_series[-1]\n",
        "      elif args.prompts_series is not None:\n",
        "        frame_prompt = args.prompts_series[frame_num]\n",
        "      else:\n",
        "        frame_prompt = []\n",
        "      \n",
        "      print(args.image_prompts_series)\n",
        "      if args.image_prompts_series is not None and frame_num >= len(args.image_prompts_series):\n",
        "        image_prompt = args.image_prompts_series[-1]\n",
        "      elif args.image_prompts_series is not None:\n",
        "        image_prompt = args.image_prompts_series[frame_num]\n",
        "      else:\n",
        "        image_prompt = []\n",
        "\n",
        "      print(f'Frame {frame_num} Prompt: {frame_prompt}')\n",
        "\n",
        "      model_stats = []\n",
        "      for clip_model in clip_models:\n",
        "            cutn = 16\n",
        "            model_stat = {\"clip_model\":None,\"target_embeds\":[],\"make_cutouts\":None,\"weights\":[]}\n",
        "            model_stat[\"clip_model\"] = clip_model\n",
        "            \n",
        "            \n",
        "            for prompt in frame_prompt:\n",
        "                txt, weight = parse_prompt(prompt)\n",
        "                txt = clip_model.encode_text(clip.tokenize(prompt).to(device)).float()\n",
        "                \n",
        "                if args.fuzzy_prompt:\n",
        "                    for i in range(25):\n",
        "                        model_stat[\"target_embeds\"].append((txt + torch.randn(txt.shape).cuda() * args.rand_mag).clamp(0,1))\n",
        "                        model_stat[\"weights\"].append(weight)\n",
        "                else:\n",
        "                    model_stat[\"target_embeds\"].append(txt)\n",
        "                    model_stat[\"weights\"].append(weight)\n",
        "        \n",
        "            if image_prompt:\n",
        "              model_stat[\"make_cutouts\"] = MakeCutouts(clip_model.visual.input_resolution, cutn, skip_augs=skip_augs) \n",
        "              for prompt in image_prompt:\n",
        "                  path, weight = parse_prompt(prompt)\n",
        "                  img = Image.open(fetch(path)).convert('RGB')\n",
        "                  img = TF.resize(img, min(side_x, side_y, *img.size), T.InterpolationMode.LANCZOS)\n",
        "                  batch = model_stat[\"make_cutouts\"](TF.to_tensor(img).to(device).unsqueeze(0).mul(2).sub(1))\n",
        "                  embed = clip_model.encode_image(normalize(batch)).float()\n",
        "                  if fuzzy_prompt:\n",
        "                      for i in range(25):\n",
        "                          model_stat[\"target_embeds\"].append((embed + torch.randn(embed.shape).cuda() * rand_mag).clamp(0,1))\n",
        "                          weights.extend([weight / cutn] * cutn)\n",
        "                  else:\n",
        "                      model_stat[\"target_embeds\"].append(embed)\n",
        "                      model_stat[\"weights\"].extend([weight / cutn] * cutn)\n",
        "        \n",
        "            model_stat[\"target_embeds\"] = torch.cat(model_stat[\"target_embeds\"])\n",
        "            model_stat[\"weights\"] = torch.tensor(model_stat[\"weights\"], device=device)\n",
        "            if model_stat[\"weights\"].sum().abs() < 1e-3:\n",
        "                raise RuntimeError('The weights must not sum to 0.')\n",
        "            model_stat[\"weights\"] /= model_stat[\"weights\"].sum().abs()\n",
        "            model_stats.append(model_stat)\n",
        "  \n",
        "      init = None\n",
        "      if init_image is not None:\n",
        "          init = Image.open(fetch(init_image)).convert('RGB')\n",
        "          #zippy noise injector csa\n",
        "          if args.noise_injector == True and args.animation_mode == \"3D\":\n",
        "            init_to_noise = cv2.imread(init_image)\n",
        "            if args.copy_palette == True:\n",
        "              palette_source_img = cv2.imread(args.copy_palette_image)\n",
        "              init_to_noise = exposure.match_histograms(init_to_noise, palette_source_img, multichannel=True) \n",
        "            edge_mix = get_edge_mix(center_crop(init_to_noise,.33))\n",
        "            print('edge mix is...',round(edge_mix,1))\n",
        "            if edge_mix < args.noise_injector_threshold:\n",
        "              init_to_noise = add_noise(init_to_noise,args.noise_injector_mix,args.noise_injector_px_size,\n",
        "                                        args.noise_injector_spice,args.noise_injector_mask_feather,\n",
        "                                        args.noise_injector_sigma) \n",
        "              print('edge mix was below threshold, noise was added')\n",
        "              #ratio = np.amax(init_to_noise) / 256       \n",
        "              #converted_noised_init = (init_to_noise / ratio).astype('uint8')\n",
        "              #recolored_noised_init = cv2.cvtColor(converted_noised_init,cv2.COLOR_BGR2RGB)\n",
        "              #init = Image.fromarray(recolored_noised_init)\n",
        "            else:\n",
        "              print('edge mix was above threshold, no noise was added')\n",
        "            #now convert init_to_noise back to PIL img format\n",
        "            ratio = np.amax(init_to_noise) / 256       \n",
        "            converted_noised_init = (init_to_noise / ratio).astype('uint8')\n",
        "            recolored_noised_init = cv2.cvtColor(converted_noised_init,cv2.COLOR_BGR2RGB)\n",
        "            init = Image.fromarray(recolored_noised_init)         \n",
        "          #end noise injector\n",
        "          init = init.resize((args.side_x, args.side_y), Image.LANCZOS)\n",
        "          init = TF.to_tensor(init).to(device).unsqueeze(0).mul(2).sub(1)\n",
        "      \n",
        "      if args.perlin_init:\n",
        "          if args.perlin_mode == 'color':\n",
        "              init = create_perlin_noise([1.5**-i*0.5 for i in range(12)], 1, 1, False)\n",
        "              init2 = create_perlin_noise([1.5**-i*0.5 for i in range(8)], 4, 4, False)\n",
        "          elif args.perlin_mode == 'gray':\n",
        "            init = create_perlin_noise([1.5**-i*0.5 for i in range(12)], 1, 1, True)\n",
        "            init2 = create_perlin_noise([1.5**-i*0.5 for i in range(8)], 4, 4, True)\n",
        "          else:\n",
        "            init = create_perlin_noise([1.5**-i*0.5 for i in range(12)], 1, 1, False)\n",
        "            init2 = create_perlin_noise([1.5**-i*0.5 for i in range(8)], 4, 4, True)\n",
        "          # init = TF.to_tensor(init).add(TF.to_tensor(init2)).div(2).to(device)\n",
        "          init = TF.to_tensor(init).add(TF.to_tensor(init2)).div(2).to(device).unsqueeze(0).mul(2).sub(1)\n",
        "          del init2\n",
        "  \n",
        "      cur_t = None\n",
        "  \n",
        "      def cond_fn(x, t, y=None):\n",
        "          with torch.enable_grad():\n",
        "              x_is_NaN = False\n",
        "              x = x.detach().requires_grad_()\n",
        "              n = x.shape[0]\n",
        "              if use_secondary_model is True:\n",
        "                alpha = torch.tensor(diffusion.sqrt_alphas_cumprod[cur_t], device=device, dtype=torch.float32)\n",
        "                sigma = torch.tensor(diffusion.sqrt_one_minus_alphas_cumprod[cur_t], device=device, dtype=torch.float32)\n",
        "                cosine_t = alpha_sigma_to_t(alpha, sigma)\n",
        "                out = secondary_model(x, cosine_t[None].repeat([n])).pred\n",
        "                fac = diffusion.sqrt_one_minus_alphas_cumprod[cur_t]\n",
        "                x_in = out * fac + x * (1 - fac)\n",
        "                x_in_grad = torch.zeros_like(x_in)\n",
        "              else:\n",
        "                my_t = torch.ones([n], device=device, dtype=torch.long) * cur_t\n",
        "                out = diffusion.p_mean_variance(model, x, my_t, clip_denoised=False, model_kwargs={'y': y})\n",
        "                fac = diffusion.sqrt_one_minus_alphas_cumprod[cur_t]\n",
        "                x_in = out['pred_xstart'] * fac + x * (1 - fac)\n",
        "                x_in_grad = torch.zeros_like(x_in)\n",
        "              for model_stat in model_stats:\n",
        "                for i in range(args.cutn_batches):\n",
        "                    t_int = int(t.item())+1 #errors on last step without +1, need to find source\n",
        "                    #when using SLIP Base model the dimensions need to be hard coded to avoid AttributeError: 'VisionTransformer' object has no attribute 'input_resolution'\n",
        "                    try:\n",
        "                        input_resolution=model_stat[\"clip_model\"].visual.input_resolution\n",
        "                    except:\n",
        "                        input_resolution=224\n",
        "\n",
        "                    cuts = MakeCutoutsDango(input_resolution,\n",
        "                            Overview= args.cut_overview[1000-t_int], \n",
        "                            InnerCrop = args.cut_innercut[1000-t_int], IC_Size_Pow=args.cut_ic_pow, IC_Grey_P = args.cut_icgray_p[1000-t_int]\n",
        "                            )\n",
        "                    clip_in = normalize(cuts(x_in.add(1).div(2)))\n",
        "                    image_embeds = model_stat[\"clip_model\"].encode_image(clip_in).float()\n",
        "                    dists = spherical_dist_loss(image_embeds.unsqueeze(1), model_stat[\"target_embeds\"].unsqueeze(0))\n",
        "                    dists = dists.view([args.cut_overview[1000-t_int]+args.cut_innercut[1000-t_int], n, -1])\n",
        "                    losses = dists.mul(model_stat[\"weights\"]).sum(2).mean(0)\n",
        "                    loss_values.append(losses.sum().item()) # log loss, probably shouldn't do per cutn_batch\n",
        "                    x_in_grad += torch.autograd.grad(losses.sum() * clip_guidance_scale, x_in)[0] / cutn_batches\n",
        "              tv_losses = tv_loss(x_in)\n",
        "              if use_secondary_model is True:\n",
        "                range_losses = range_loss(out)\n",
        "              else:\n",
        "                range_losses = range_loss(out['pred_xstart'])\n",
        "              #sat_losses = torch.abs(x_in - x_in.clamp(min=-1,max=1)).mean()\n",
        "              #updated sat_losses function.. csa uses sat_scale_buffer\n",
        "              sat_losses = torch.abs(x_in - x_in.clamp(min=-1+args.sat_scale_buffer,max=1-args.sat_scale_buffer)).mean()\n",
        "              if sat_scale_buffer > 0:\n",
        "                sat_losses = (sat_losses/sat_scale_buffer)**2 # \n",
        "                sat_losses = sat_losses.clamp(min=0,max=2)#limit to 2 max to avoid nonlinearity\n",
        "              loss = tv_losses.sum() * tv_scale + range_losses.sum() * range_scale + sat_losses.sum() * sat_scale\n",
        "              if init is not None and init_scale:\n",
        "                  init_losses = lpips_model(x_in, init)\n",
        "                  loss = loss + init_losses.sum() * init_scale\n",
        "              x_in_grad += torch.autograd.grad(loss, x_in)[0]\n",
        "              if torch.isnan(x_in_grad).any()==False:\n",
        "                  grad = -torch.autograd.grad(x_in, x, x_in_grad)[0]\n",
        "              else:\n",
        "                # print(\"NaN'd\")\n",
        "                x_is_NaN = True\n",
        "                grad = torch.zeros_like(x)\n",
        "          if args.clamp_grad and x_is_NaN == False:\n",
        "              magnitude = grad.square().mean().sqrt()\n",
        "              return grad * magnitude.clamp(max=args.clamp_max) / magnitude  #min=-0.02, min=-clamp_max, \n",
        "          return grad\n",
        "  \n",
        "      if args.diffusion_sampling_mode == 'ddim':\n",
        "          sample_fn = diffusion.ddim_sample_loop_progressive\n",
        "      else:\n",
        "          sample_fn = diffusion.plms_sample_loop_progressive\n",
        "\n",
        "\n",
        "      image_display = Output()\n",
        "      for i in range(args.n_batches):\n",
        "          if args.animation_mode == 'None':\n",
        "            display.clear_output(wait=True)\n",
        "            batchBar = tqdm(range(args.n_batches), desc =\"Batches\")\n",
        "            batchBar.n = i\n",
        "            batchBar.refresh()\n",
        "          print('')\n",
        "          display.display(image_display)\n",
        "          gc.collect()\n",
        "          torch.cuda.empty_cache()\n",
        "          cur_t = diffusion.num_timesteps - skip_steps - 1\n",
        "          total_steps = cur_t\n",
        "          cur_t -= skip_end_steps\n",
        "\n",
        "          if perlin_init:\n",
        "              init = regen_perlin()\n",
        "\n",
        "          if args.diffusion_sampling_mode == 'ddim':\n",
        "              samples = sample_fn(\n",
        "                  model,\n",
        "                  (batch_size, 3, args.side_y, args.side_x),\n",
        "                  clip_denoised=clip_denoised,\n",
        "                  model_kwargs={},\n",
        "                  cond_fn=cond_fn,\n",
        "                  progress=True,\n",
        "                  skip_timesteps=skip_steps,\n",
        "                  init_image=init,\n",
        "                  randomize_class=randomize_class,\n",
        "                  eta=eta,\n",
        "              )\n",
        "          else:\n",
        "              samples = sample_fn(\n",
        "                  model,\n",
        "                  (batch_size, 3, args.side_y, args.side_x),\n",
        "                  clip_denoised=clip_denoised,\n",
        "                  model_kwargs={},\n",
        "                  cond_fn=cond_fn,\n",
        "                  progress=True,\n",
        "                  skip_timesteps=skip_steps,\n",
        "                  init_image=init,\n",
        "                  randomize_class=randomize_class,\n",
        "                  order=2,\n",
        "              )\n",
        "          \n",
        "          \n",
        "          # with run_display:\n",
        "          # display.clear_output(wait=True)\n",
        "          for j, sample in enumerate(samples):    \n",
        "            cur_t -= 1\n",
        "            intermediateStep = False\n",
        "            if args.steps_per_checkpoint is not None:\n",
        "                if j % steps_per_checkpoint == 0 and j > 0:\n",
        "                  intermediateStep = True\n",
        "            elif j in args.intermediate_saves:\n",
        "              intermediateStep = True\n",
        "            with image_display:\n",
        "              if j % args.display_rate == 0 or cur_t == -1 or intermediateStep == True:\n",
        "                  for k, image in enumerate(sample['pred_xstart']):\n",
        "                      # tqdm.write(f'Batch {i}, step {j}, output {k}:')\n",
        "                      current_time = datetime.now().strftime('%y%m%d-%H%M%S_%f')\n",
        "                      percent = math.ceil(j/total_steps*100)\n",
        "                      if args.n_batches > 0:\n",
        "                        #if intermediates are saved to the subfolder, don't append a step or percentage to the name\n",
        "                        if cur_t == -1 and args.intermediates_in_subfolder is True:\n",
        "                          save_num = f'{frame_num:04}' if animation_mode != \"None\" else i\n",
        "                          filename = f'{args.batch_name}({args.batchNum})_{save_num}.png'\n",
        "                        else:\n",
        "                          #If we're working with percentages, append it\n",
        "                          if args.steps_per_checkpoint is not None:\n",
        "                            filename = f'{args.batch_name}({args.batchNum})_{i:04}-{percent:02}%.png'\n",
        "                          # Or else, iIf we're working with specific steps, append those\n",
        "                          else:\n",
        "                            filename = f'{args.batch_name}({args.batchNum})_{i:04}-{j:03}.png'\n",
        "                      #Optional Soft Limiter\n",
        "                      if args.soft_limiter_on == True:\n",
        "                          #zippy's soft limiter\n",
        "                          image=(soft_limit(image)+1)/2\n",
        "                          image = TF.to_pil_image(image)\n",
        "                      else:\n",
        "                          #default clamping behavior\n",
        "                          #clamp values to between 0 and 1\n",
        "                          image = TF.to_pil_image(image.add(1).div(2).clamp(0, 1))\n",
        "                      if j % args.display_rate == 0 or cur_t == -1:\n",
        "                        image.save('progress.png')\n",
        "                        display.clear_output(wait=True)\n",
        "                        display.display(display.Image('progress.png'))\n",
        "                        #display histogram csa\n",
        "                        if args.display_histogram:\n",
        "                          hist_img=cv2.imread(\"progress.png\")\n",
        "                          chans = cv2.split(hist_img)\n",
        "                          colors = (\"b\", \"g\", \"r\")\n",
        "                          plt.figure()\n",
        "                          #plt.title(f\"B/G/R at {percent}%\")\n",
        "                          plt.xlabel(f\"{percent}% complete\")\n",
        "                          plt.ylabel(\"% of Pixels\")\n",
        "                          plt.gca().spines['right'].set_visible(False)\n",
        "                          plt.gca().spines['left'].set_visible(False)\n",
        "                          plt.gca().spines['bottom'].set_visible(False)\n",
        "                          plt.gca().yaxis.set_major_formatter(StrMethodFormatter('{x:,.2%}')) # 2 decimal places\n",
        "                          # loop over the image channels\n",
        "                          for (chan, color) in zip(chans, colors):\n",
        "                            # create a histogram for the current channel and plot it\n",
        "                            hist = cv2.calcHist([chan], [0], None, [256], [0, 256])\n",
        "                            hist /= hist.sum()\n",
        "                            plt.plot(hist, color=color)\n",
        "                            plt.xlim([0, 256])                      \n",
        "                      if args.steps_per_checkpoint is not None:\n",
        "                        if j % args.steps_per_checkpoint == 0 and j > 0:\n",
        "                          if args.intermediates_in_subfolder is True:\n",
        "                            image.save(f'{partialFolder}/{filename}')\n",
        "                          else:\n",
        "                            image.save(f'{batchFolder}/{filename}')\n",
        "                      else:\n",
        "                        if j in args.intermediate_saves:\n",
        "                          if args.intermediates_in_subfolder is True:\n",
        "                            image.save(f'{partialFolder}/{filename}')\n",
        "                          else:\n",
        "                            image.save(f'{batchFolder}/{filename}')\n",
        "                      if cur_t == -1:\n",
        "                        if frame_num == 0:\n",
        "                          save_settings()\n",
        "                        if args.animation_mode != \"None\":\n",
        "                          image.save('prevFrame.png')\n",
        "                        image.save(f'{batchFolder}/{filename}')\n",
        "                        if args.animation_mode == \"3D\":\n",
        "                          # If turbo, save a blended image\n",
        "                          if turbo_mode and frame_num > 0:\n",
        "                            # Mix new image with prevFrameScaled\n",
        "                            blend_factor = (1)/int(turbo_steps)\n",
        "                            newFrame = cv2.imread('prevFrame.png') # This is already updated..\n",
        "                            prev_frame_warped = cv2.imread('prevFrameScaled.png')\n",
        "                            blendedImage = cv2.addWeighted(newFrame, blend_factor, prev_frame_warped, (1-blend_factor), 0.0)\n",
        "                            cv2.imwrite(f'{batchFolder}/{filename}',blendedImage)\n",
        "                          else:\n",
        "                            image.save(f'{batchFolder}/{filename}')\n",
        "\n",
        "                          if vr_mode:\n",
        "                            generate_eye_views(TRANSLATION_SCALE, batchFolder, filename, frame_num, midas_model, midas_transform)\n",
        "\n",
        "                        # if frame_num != args.max_frames-1:\n",
        "                        #   display.clear_output()\n",
        "            if cur_t < -1 : break # this is mainly used to account for skip_end_steps\n",
        "          \n",
        "          plt.plot(np.array(loss_values), 'r')\n",
        "\n",
        "def generate_eye_views(trans_scale,batchFolder,filename,frame_num,midas_model, midas_transform):\n",
        "   for i in range(2):\n",
        "      theta = vr_eye_angle * (math.pi/180)\n",
        "      ray_origin = math.cos(theta) * vr_ipd / 2 * (-1.0 if i==0 else 1.0)\n",
        "      ray_rotation = (theta if i==0 else -theta)\n",
        "      translate_xyz = [-(ray_origin)*trans_scale, 0,0]\n",
        "      rotate_xyz = [0, (ray_rotation), 0]\n",
        "      rot_mat = p3dT.euler_angles_to_matrix(torch.tensor(rotate_xyz, device=device), \"XYZ\").unsqueeze(0)\n",
        "      transformed_image = dxf.transform_image_3d(f'{batchFolder}/{filename}', midas_model, midas_transform, DEVICE,\n",
        "                                                      rot_mat, translate_xyz, args.near_plane, args.far_plane,\n",
        "                                                      args.fov, padding_mode=args.padding_mode,\n",
        "                                                      sampling_mode=args.sampling_mode, midas_weight=args.midas_weight,spherical=True)\n",
        "      eye_file_path = batchFolder+f\"/frame_{frame_num:04}\" + ('_l' if i==0 else '_r')+'.png'\n",
        "      transformed_image.save(eye_file_path)\n",
        "\n",
        "def save_settings():\n",
        "  setting_list = {\n",
        "    'text_prompts': text_prompts,\n",
        "    'image_prompts': image_prompts,\n",
        "    'clip_guidance_scale': clip_guidance_scale,\n",
        "    'tv_scale': tv_scale,\n",
        "    'range_scale': range_scale,\n",
        "    'sat_scale': sat_scale,\n",
        "    'sat_scale_buffer': sat_scale_buffer,\n",
        "    # 'cutn': cutn,\n",
        "    'cutn_batches': cutn_batches,\n",
        "    'soft_limiter_on': soft_limiter_on,\n",
        "    'soft_limiter_knee': soft_limiter_knee,\n",
        "    'init_image': init_image,\n",
        "    'init_scale': init_scale,\n",
        "    'skip_steps': skip_steps,\n",
        "    'skip_end_steps': skip_end_steps,\n",
        "    # 'zoom_per_frame': zoom_per_frame,\n",
        "    'max_frames': max_frames,\n",
        "    'interp_spline': interp_spline,\n",
        "    # 'rotation_per_frame': rotation_per_frame,\n",
        "    'frames_scale': frames_scale,\n",
        "    'frames_skip_steps': frames_skip_steps,\n",
        "    'perlin_init': perlin_init,\n",
        "    'perlin_mode': perlin_mode,\n",
        "    'skip_augs': skip_augs,\n",
        "    'randomize_class': randomize_class,\n",
        "    'clip_denoised': clip_denoised,\n",
        "    'clamp_grad': clamp_grad,\n",
        "    'clamp_max': clamp_max,\n",
        "    'seed': seed,\n",
        "    'fuzzy_prompt': fuzzy_prompt,\n",
        "    'rand_mag': rand_mag,\n",
        "    'eta': eta,\n",
        "    'width': width_height[0],\n",
        "    'height': width_height[1],\n",
        "    'diffusion_model': diffusion_model,\n",
        "    'use_secondary_model': use_secondary_model,\n",
        "    'steps': steps,\n",
        "    'diffusion_steps': diffusion_steps,\n",
        "    'diffusion_sampling_mode': diffusion_sampling_mode,\n",
        "    'ViTB32': ViTB32,\n",
        "    'ViTB16': ViTB16,\n",
        "    'ViTL14': ViTL14,\n",
        "    'RN101': RN101,\n",
        "    'RN50': RN50,\n",
        "    'RN50x4': RN50x4,\n",
        "    'RN50x16': RN50x16,\n",
        "    'RN50x64': RN50x64,\n",
        "    'cut_overview': str(cut_overview),\n",
        "    'cut_innercut': str(cut_innercut),\n",
        "    'cut_ic_pow': cut_ic_pow,\n",
        "    'cut_icgray_p': str(cut_icgray_p),\n",
        "    'key_frames': key_frames,\n",
        "    'max_frames': max_frames,\n",
        "    'angle': angle,\n",
        "    'zoom': zoom,\n",
        "    'translation_x': translation_x,\n",
        "    'translation_y': translation_y,\n",
        "    'translation_z': translation_z,\n",
        "    'rotation_3d_x': rotation_3d_x,\n",
        "    'rotation_3d_y': rotation_3d_y,\n",
        "    'rotation_3d_z': rotation_3d_z,\n",
        "    'midas_depth_model': midas_depth_model,\n",
        "    'midas_weight': midas_weight,\n",
        "    'near_plane': near_plane,\n",
        "    'far_plane': far_plane,\n",
        "    'fov': fov,\n",
        "    'padding_mode': padding_mode,\n",
        "    'sampling_mode': sampling_mode,\n",
        "    'video_init_path':video_init_path,\n",
        "    'extract_nth_frame':extract_nth_frame,\n",
        "    'video_init_seed_continuity': video_init_seed_continuity,\n",
        "    'turbo_mode':turbo_mode,\n",
        "    'turbo_steps':turbo_steps,\n",
        "    'turbo_preroll':turbo_preroll,\n",
        "    'noise_injector': noise_injector,\n",
        "    'noise_injector_threshold': noise_injector_threshold,\n",
        "    'noise_injector_px_size': noise_injector_px_size,\n",
        "    'noise_injector_sigma': noise_injector_sigma,\n",
        "    'noise_injector_spice': noise_injector_spice,\n",
        "    'noise_injector_mask_feather': noise_injector_mask_feather,\n",
        "    'copy_palette': copy_palette,\n",
        "    'copy_palette_image': copy_palette_image,\n",
        "  }\n",
        "  # print('Settings:', setting_list)\n",
        "  with open(f\"{batchFolder}/{batch_name}({batchNum})_settings.txt\", \"w+\") as f:   #save settings\n",
        "    json.dump(setting_list, f, ensure_ascii=False, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cellView": "form",
        "id": "DefSecModel"
      },
      "outputs": [],
      "source": [
        "#@title 1.6 Define the secondary diffusion model\n",
        "\n",
        "def append_dims(x, n):\n",
        "    return x[(Ellipsis, *(None,) * (n - x.ndim))]\n",
        "\n",
        "\n",
        "def expand_to_planes(x, shape):\n",
        "    return append_dims(x, len(shape)).repeat([1, 1, *shape[2:]])\n",
        "\n",
        "\n",
        "def alpha_sigma_to_t(alpha, sigma):\n",
        "    return torch.atan2(sigma, alpha) * 2 / math.pi\n",
        "\n",
        "\n",
        "def t_to_alpha_sigma(t):\n",
        "    return torch.cos(t * math.pi / 2), torch.sin(t * math.pi / 2)\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DiffusionOutput:\n",
        "    v: torch.Tensor\n",
        "    pred: torch.Tensor\n",
        "    eps: torch.Tensor\n",
        "\n",
        "\n",
        "class ConvBlock(nn.Sequential):\n",
        "    def __init__(self, c_in, c_out):\n",
        "        super().__init__(\n",
        "            nn.Conv2d(c_in, c_out, 3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "\n",
        "class SkipBlock(nn.Module):\n",
        "    def __init__(self, main, skip=None):\n",
        "        super().__init__()\n",
        "        self.main = nn.Sequential(*main)\n",
        "        self.skip = skip if skip else nn.Identity()\n",
        "\n",
        "    def forward(self, input):\n",
        "        return torch.cat([self.main(input), self.skip(input)], dim=1)\n",
        "\n",
        "\n",
        "class FourierFeatures(nn.Module):\n",
        "    def __init__(self, in_features, out_features, std=1.):\n",
        "        super().__init__()\n",
        "        assert out_features % 2 == 0\n",
        "        self.weight = nn.Parameter(torch.randn([out_features // 2, in_features]) * std)\n",
        "\n",
        "    def forward(self, input):\n",
        "        f = 2 * math.pi * input @ self.weight.T\n",
        "        return torch.cat([f.cos(), f.sin()], dim=-1)\n",
        "\n",
        "\n",
        "class SecondaryDiffusionImageNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        c = 64  # The base channel count\n",
        "\n",
        "        self.timestep_embed = FourierFeatures(1, 16)\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            ConvBlock(3 + 16, c),\n",
        "            ConvBlock(c, c),\n",
        "            SkipBlock([\n",
        "                nn.AvgPool2d(2),\n",
        "                ConvBlock(c, c * 2),\n",
        "                ConvBlock(c * 2, c * 2),\n",
        "                SkipBlock([\n",
        "                    nn.AvgPool2d(2),\n",
        "                    ConvBlock(c * 2, c * 4),\n",
        "                    ConvBlock(c * 4, c * 4),\n",
        "                    SkipBlock([\n",
        "                        nn.AvgPool2d(2),\n",
        "                        ConvBlock(c * 4, c * 8),\n",
        "                        ConvBlock(c * 8, c * 4),\n",
        "                        nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
        "                    ]),\n",
        "                    ConvBlock(c * 8, c * 4),\n",
        "                    ConvBlock(c * 4, c * 2),\n",
        "                    nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
        "                ]),\n",
        "                ConvBlock(c * 4, c * 2),\n",
        "                ConvBlock(c * 2, c),\n",
        "                nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
        "            ]),\n",
        "            ConvBlock(c * 2, c),\n",
        "            nn.Conv2d(c, 3, 3, padding=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, input, t):\n",
        "        timestep_embed = expand_to_planes(self.timestep_embed(t[:, None]), input.shape)\n",
        "        v = self.net(torch.cat([input, timestep_embed], dim=1))\n",
        "        alphas, sigmas = map(partial(append_dims, n=v.ndim), t_to_alpha_sigma(t))\n",
        "        pred = input * alphas - v * sigmas\n",
        "        eps = input * sigmas + v * alphas\n",
        "        return DiffusionOutput(v, pred, eps)\n",
        "\n",
        "\n",
        "class SecondaryDiffusionImageNet2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        c = 64  # The base channel count\n",
        "        cs = [c, c * 2, c * 2, c * 4, c * 4, c * 8]\n",
        "\n",
        "        self.timestep_embed = FourierFeatures(1, 16)\n",
        "        self.down = nn.AvgPool2d(2)\n",
        "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            ConvBlock(3 + 16, cs[0]),\n",
        "            ConvBlock(cs[0], cs[0]),\n",
        "            SkipBlock([\n",
        "                self.down,\n",
        "                ConvBlock(cs[0], cs[1]),\n",
        "                ConvBlock(cs[1], cs[1]),\n",
        "                SkipBlock([\n",
        "                    self.down,\n",
        "                    ConvBlock(cs[1], cs[2]),\n",
        "                    ConvBlock(cs[2], cs[2]),\n",
        "                    SkipBlock([\n",
        "                        self.down,\n",
        "                        ConvBlock(cs[2], cs[3]),\n",
        "                        ConvBlock(cs[3], cs[3]),\n",
        "                        SkipBlock([\n",
        "                            self.down,\n",
        "                            ConvBlock(cs[3], cs[4]),\n",
        "                            ConvBlock(cs[4], cs[4]),\n",
        "                            SkipBlock([\n",
        "                                self.down,\n",
        "                                ConvBlock(cs[4], cs[5]),\n",
        "                                ConvBlock(cs[5], cs[5]),\n",
        "                                ConvBlock(cs[5], cs[5]),\n",
        "                                ConvBlock(cs[5], cs[4]),\n",
        "                                self.up,\n",
        "                            ]),\n",
        "                            ConvBlock(cs[4] * 2, cs[4]),\n",
        "                            ConvBlock(cs[4], cs[3]),\n",
        "                            self.up,\n",
        "                        ]),\n",
        "                        ConvBlock(cs[3] * 2, cs[3]),\n",
        "                        ConvBlock(cs[3], cs[2]),\n",
        "                        self.up,\n",
        "                    ]),\n",
        "                    ConvBlock(cs[2] * 2, cs[2]),\n",
        "                    ConvBlock(cs[2], cs[1]),\n",
        "                    self.up,\n",
        "                ]),\n",
        "                ConvBlock(cs[1] * 2, cs[1]),\n",
        "                ConvBlock(cs[1], cs[0]),\n",
        "                self.up,\n",
        "            ]),\n",
        "            ConvBlock(cs[0] * 2, cs[0]),\n",
        "            nn.Conv2d(cs[0], 3, 3, padding=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, input, t):\n",
        "        timestep_embed = expand_to_planes(self.timestep_embed(t[:, None]), input.shape)\n",
        "        v = self.net(torch.cat([input, timestep_embed], dim=1))\n",
        "        alphas, sigmas = map(partial(append_dims, n=v.ndim), t_to_alpha_sigma(t))\n",
        "        pred = input * alphas - v * sigmas\n",
        "        eps = input * sigmas + v * alphas\n",
        "        return DiffusionOutput(v, pred, eps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiffClipSetTop"
      },
      "source": [
        "# 2. Diffusion and CLIP model settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ModelSettings"
      },
      "outputs": [],
      "source": [
        "#@markdown ####**Models Settings:**\n",
        "diffusion_model = \"pixelartdiffusion4k\" #@param [\"pixel_art_diffusion_hard_256\", \"pixel_art_diffusion_soft_256\", \"pixelartdiffusion4k\"]\n",
        "\n",
        "#@markdown If you enable the secondary model, beware: secondary *REALLY* likes making everything it can into buildings.\n",
        "use_secondary_model = False #@param {type: 'boolean'}\n",
        "diffusion_sampling_mode = 'ddim' #@param ['plms','ddim'] \n",
        "\n",
        "\n",
        "#@markdown #####**CLIP settings:**\n",
        "ViTB32 = True #@param{type:\"boolean\"}\n",
        "ViTB16 = True #@param{type:\"boolean\"}\n",
        "ViTL14 = False #@param{type:\"boolean\"}\n",
        "ViTL14_336 = False #@param{type:\"boolean\"}\n",
        "RN101 = False #@param{type:\"boolean\"}\n",
        "RN50 = True #@param{type:\"boolean\"}\n",
        "RN50x4 = False #@param{type:\"boolean\"}\n",
        "RN50x16 = False #@param{type:\"boolean\"}\n",
        "RN50x64 = False #@param{type:\"boolean\"}\n",
        "\n",
        "#@markdown If you're having issues with model downloads, check this to compare SHA's:\n",
        "check_model_SHA = False #@param{type:\"boolean\"}\n",
        "use_checkpoint = True #@param {type: 'boolean'}\n",
        "model_pixel_art_diffusion_hard_256_SHA = 'be4a9de943ec06eef32c65a1008c60ad017723a4d35dc13169c66bb322234161'\n",
        "model_pixel_art_diffusion_soft_256_SHA = 'd321590e46b679bf6def1f1914b47c89e762c76f19ab3e3392c8ca07c791039c'\n",
        "model_pixelartdiffusion4k_SHA = 'a1ba4f13f6dabb72b1064f15d8ae504d98d6192ad343572cc416deda7cccac30'\n",
        "model_secondary_SHA = '983e3de6f95c88c81b2ca7ebb2c217933be1973b1ff058776b970f901584613a'\n",
        "\n",
        "model_pixel_art_diffusion_hard_256_link = 'https://huggingface.co/KaliYuga/pixel_art_diffusion_hard_256/resolve/main/pixel_art_diffusion_hard_256.pt'\n",
        "model_pixel_art_diffusion_soft_256_link = 'https://huggingface.co/KaliYuga/pixel_art_diffusion_soft_256/resolve/main/pixel_art_diffusion_soft_256.pt'\n",
        "model_pixelartdiffusion4k_link = 'https://huggingface.co/KaliYuga/pixelartdiffusion4k/resolve/main/pixelartdiffusion4k.pt'\n",
        "model_secondary_link = 'https://v-diffusion.s3.us-west-2.amazonaws.com/secondary_model_imagenet_2.pth'\n",
        "\n",
        "model_pixel_art_diffusion_hard_256_path = f'{model_path}/pixel_art_diffusion_hard_256.pt'\n",
        "model_pixel_art_diffusion_soft_256_path = f'{model_path}/pixel_art_diffusion_soft_256.pt'\n",
        "model_pixelartdiffusion4k_path = f'{model_path}/pixelartdiffusion4k.pt'\n",
        "model_secondary_path = f'{model_path}/secondary_model_imagenet_2.pth'\n",
        "\n",
        "model_pixel_art_diffusion_hard_256_downloaded = False\n",
        "model_pixel_art_diffusion_soft_256_downloaded = False\n",
        "model_pixelartdiffusion4k_downloaded = False\n",
        "model_secondary_downloaded = False\n",
        "\n",
        "\n",
        "# Download the diffusion model\n",
        "if diffusion_model == 'pixel_art_diffusion_hard_256':\n",
        "  if os.path.exists(model_pixel_art_diffusion_hard_256_path) and check_model_SHA:\n",
        "    print('Checking 256 Pixel Art Hard Model File')\n",
        "    with open(model_pixel_art_diffusion_hard_256_path,\"rb\") as f:\n",
        "        bytes = f.read() \n",
        "        hash = hashlib.sha256(bytes).hexdigest();\n",
        "    if hash == model_pixel_art_diffusion_hard_256_SHA:\n",
        "      print('256 Pixel Art Hard Model SHA matches')\n",
        "      model_pixel_art_diffusion_hard_256_downloaded = True\n",
        "    else: \n",
        "      print(\"256 Pixel Art Hard Model doesn't match, redownloading...\")\n",
        "      wget(model_pixel_art_diffusion_hard_256_link, model_path)\n",
        "      model_pixel_art_diffusion_hard_256_downloaded = True\n",
        "  elif os.path.exists(model_pixel_art_diffusion_hard_256_path) and not check_model_SHA or model_pixel_art_diffusion_hard_256_downloaded == True:\n",
        "    print('256 Pixel Art Hard Model already downloaded, check check_model_SHA if the file is corrupt')\n",
        "  else:  \n",
        "    wget(model_pixel_art_diffusion_hard_256_link, model_path)\n",
        "    model_pixel_art_diffusion_hard_256_downloaded = True\n",
        "\n",
        "if diffusion_model == 'pixel_art_diffusion_soft_256':\n",
        "  if os.path.exists(model_pixel_art_diffusion_soft_256_path) and check_model_SHA:\n",
        "    print('Checking 256 Pixel Art Soft Model File')\n",
        "    with open(model_pixel_art_diffusion_soft_256_path,\"rb\") as f:\n",
        "        bytes = f.read() \n",
        "        hash = hashlib.sha256(bytes).hexdigest();\n",
        "    if hash == model_pixel_art_diffusion_soft_256_SHA:\n",
        "      print('256 Pixel Art Soft Model SHA matches')\n",
        "      model_pixel_art_diffusion_soft_256_downloaded = True\n",
        "    else: \n",
        "      print(\"256 Pixel Art Soft Model doesn't match, redownloading...\")\n",
        "      wget(model_pixel_art_diffusion_soft_256_link, model_path)\n",
        "      model_pixel_art_diffusion_soft_256_downloaded = True\n",
        "  elif os.path.exists(model_pixel_art_diffusion_soft_256_path) and not check_model_SHA or model_pixel_art_diffusion_soft_256_downloaded == True:\n",
        "    print('256 Pixel Art Soft Model already downloaded, check check_model_SHA if the file is corrupt')\n",
        "  else:  \n",
        "    wget(model_pixel_art_diffusion_soft_256_link, model_path)\n",
        "    model_pixel_art_diffusion_soft_256_downloaded = True\n",
        "\n",
        "if diffusion_model == 'pixelartdiffusion4k':\n",
        "  if os.path.exists(model_pixelartdiffusion4k_path) and check_model_SHA:\n",
        "    print('Checking 256 Pixel Art Soft Model File')\n",
        "    with open(model_pixelartdiffusion4k_path,\"rb\") as f:\n",
        "        bytes = f.read() \n",
        "        hash = hashlib.sha256(bytes).hexdigest();\n",
        "    if hash == model_pixelartdiffusion4k_SHA:\n",
        "      print('Pixel Art Diffusion 4k Model SHA matches')\n",
        "      model_pixelartdiffusion4k_downloaded = True\n",
        "    else: \n",
        "      print(\"Pixel Art Diffusion 4k Model doesn't match, redownloading...\")\n",
        "      wget(model_pixelartdiffusion4k_link, model_path)\n",
        "      model_pixelartdiffusion4k_downloaded = True\n",
        "  elif os.path.exists(model_pixelartdiffusion4k_path) and not check_model_SHA or model_pixelartdiffusion4k_downloaded == True:\n",
        "    print('Pixel Art Diffusion 4k Model already downloaded, check check_model_SHA if the file is corrupt')\n",
        "  else:  \n",
        "    wget(model_pixelartdiffusion4k_link, model_path)\n",
        "    model_pixelartdiffusion4k_downloaded = True\n",
        "# Download the secondary diffusion model v2\n",
        "if use_secondary_model == True:\n",
        "  if os.path.exists(model_secondary_path) and check_model_SHA:\n",
        "    print('Checking Secondary Diffusion File')\n",
        "    with open(model_secondary_path,\"rb\") as f:\n",
        "        bytes = f.read() \n",
        "        hash = hashlib.sha256(bytes).hexdigest();\n",
        "    if hash == model_secondary_SHA:\n",
        "      print('Secondary Model SHA matches')\n",
        "      model_secondary_downloaded = True\n",
        "    else:  \n",
        "      print(\"Secondary Model SHA doesn't match, redownloading...\")\n",
        "      wget(model_secondary_link, model_path)\n",
        "      model_secondary_downloaded = True\n",
        "  elif os.path.exists(model_secondary_path) and not check_model_SHA or model_secondary_downloaded == True:\n",
        "    print('Secondary Model already downloaded, check check_model_SHA if the file is corrupt')\n",
        "  else:  \n",
        "    wget(model_secondary_link, model_path)\n",
        "    model_secondary_downloaded = True\n",
        "\n",
        "model_config = model_and_diffusion_defaults()\n",
        "if diffusion_model == '512x512_diffusion_uncond_finetune_008100':\n",
        "    model_config.update({\n",
        "        'attention_resolutions': '32, 16, 8',\n",
        "        'class_cond': False,\n",
        "        'diffusion_steps': 1000, #No need to edit this, it is taken care of later.\n",
        "        'rescale_timesteps': True,\n",
        "        'timestep_respacing': 250, #No need to edit this, it is taken care of later.\n",
        "        'image_size': 512,\n",
        "        'learn_sigma': True,\n",
        "        'noise_schedule': 'linear',\n",
        "        'num_channels': 256,\n",
        "        'num_head_channels': 64,\n",
        "        'num_res_blocks': 2,\n",
        "        'resblock_updown': True,\n",
        "        'use_checkpoint': use_checkpoint,\n",
        "        'use_fp16': True,\n",
        "        'use_scale_shift_norm': True,\n",
        "    })\n",
        "elif diffusion_model == '256x256_diffusion_uncond':\n",
        "    model_config.update({\n",
        "        'attention_resolutions': '32, 16, 8',\n",
        "        'class_cond': False,\n",
        "        'diffusion_steps': 1000, #No need to edit this, it is taken care of later.\n",
        "        'rescale_timesteps': True,\n",
        "        'timestep_respacing': 250, #No need to edit this, it is taken care of later.\n",
        "        'image_size': 256,\n",
        "        'learn_sigma': True,\n",
        "        'noise_schedule': 'linear',\n",
        "        'num_channels': 256,\n",
        "        'num_head_channels': 64,\n",
        "        'num_res_blocks': 2,\n",
        "        'resblock_updown': True,\n",
        "        'use_checkpoint': use_checkpoint,\n",
        "        'use_fp16': True,\n",
        "        'use_scale_shift_norm': True,\n",
        "    })\n",
        "elif diffusion_model == 'pixel_art_diffusion_hard_256':\n",
        "    model_config.update({\n",
        "          'attention_resolutions': '16',\n",
        "          'class_cond': False,\n",
        "          'diffusion_steps': 1000,\n",
        "          'rescale_timesteps': True,\n",
        "          'timestep_respacing': 'ddim100',\n",
        "          'image_size': 256,\n",
        "          'learn_sigma': True,\n",
        "          'noise_schedule': 'linear',\n",
        "          'num_channels': 128,\n",
        "          'num_heads': 1,\n",
        "          'num_res_blocks': 2,\n",
        "          'use_checkpoint': use_checkpoint,\n",
        "          'use_fp16': True,\n",
        "          'use_scale_shift_norm': False,\n",
        "      })\n",
        "elif diffusion_model == 'pixelartdiffusion4k':\n",
        "    model_config.update({\n",
        "          'attention_resolutions': '16',\n",
        "          'class_cond': False,\n",
        "          'diffusion_steps': 1000,\n",
        "          'rescale_timesteps': True,\n",
        "          'timestep_respacing': 'ddim100',\n",
        "          'image_size': 256,\n",
        "          'learn_sigma': True,\n",
        "          'noise_schedule': 'linear',\n",
        "          'num_channels': 128,\n",
        "          'num_heads': 1,\n",
        "          'num_res_blocks': 2,\n",
        "          'use_checkpoint': use_checkpoint,\n",
        "          'use_fp16': True,\n",
        "          'use_scale_shift_norm': False,\n",
        "      })\n",
        "elif diffusion_model == 'pixel_art_diffusion_soft_256':\n",
        "    model_config.update({\n",
        "          'attention_resolutions': '16',\n",
        "          'class_cond': False,\n",
        "          'diffusion_steps': 1000,\n",
        "          'rescale_timesteps': True,\n",
        "          'timestep_respacing': 'ddim100',\n",
        "          'image_size': 256,\n",
        "          'learn_sigma': True,\n",
        "          'noise_schedule': 'linear',\n",
        "          'num_channels': 128,\n",
        "          'num_heads': 1,\n",
        "          'num_res_blocks': 2,\n",
        "          'use_checkpoint': use_checkpoint,\n",
        "          'use_fp16': True,\n",
        "          'use_scale_shift_norm': False,\n",
        "      })\n",
        "\n",
        "model_default = model_config['image_size']\n",
        "\n",
        "if use_secondary_model:\n",
        "    secondary_model = SecondaryDiffusionImageNet2()\n",
        "    secondary_model.load_state_dict(torch.load(f'{model_path}/secondary_model_imagenet_2.pth', map_location='cpu'))\n",
        "    secondary_model.eval().requires_grad_(False).to(device)\n",
        "\n",
        "clip_models = []\n",
        "if ViTB32 is True: clip_models.append(clip.load('ViT-B/32', jit=False)[0].eval().requires_grad_(False).to(device)) \n",
        "if ViTB16 is True: clip_models.append(clip.load('ViT-B/16', jit=False)[0].eval().requires_grad_(False).to(device) ) \n",
        "if ViTL14 is True: clip_models.append(clip.load('ViT-L/14', jit=False)[0].eval().requires_grad_(False).to(device) ) \n",
        "if RN50 is True: clip_models.append(clip.load('RN50', jit=False)[0].eval().requires_grad_(False).to(device))\n",
        "if RN50x4 is True: clip_models.append(clip.load('RN50x4', jit=False)[0].eval().requires_grad_(False).to(device)) \n",
        "if RN50x16 is True: clip_models.append(clip.load('RN50x16', jit=False)[0].eval().requires_grad_(False).to(device)) \n",
        "if RN50x64 is True: clip_models.append(clip.load('RN50x64', jit=False)[0].eval().requires_grad_(False).to(device)) \n",
        "if RN101 is True: clip_models.append(clip.load('RN101', jit=False)[0].eval().requires_grad_(False).to(device)) \n",
        "\n",
        "normalize = T.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
        "lpips_model = lpips.LPIPS(net='vgg').to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SettingsTop"
      },
      "source": [
        "# 3. Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "cellView": "form",
        "id": "BasicSettings"
      },
      "outputs": [],
      "source": [
        "#@markdown ####**Basic Settings:**\n",
        "batch_name = 'Pixel Art Diffusion' #@param{type: 'string'}\n",
        "steps = 75 #@param [25,50,100,150,250,500,1000]{type: 'raw', allow-input: true}\n",
        "width_height = [320, 512] #@param{type: 'raw'}\n",
        "clip_guidance_scale = 7000 #@param{type: 'number'}\n",
        "tv_scale =  0#@param{type: 'number'}\n",
        "range_scale =   300#@param{type: 'number'}\n",
        "sat_scale =   30000#@param{type: 'number'}\n",
        "sat_scale_buffer =   .05#@param{type: 'number'}\n",
        "if sat_scale_buffer < 0.0 or sat_scale_buffer > .2:\n",
        "  sat_scale_buffer = 0\n",
        "  print('sat_scale_buffer out of range. Automatically reset to 0.0')\n",
        "cutn_batches = 4  #@param{type: 'number'}\n",
        "skip_augs = False#@param{type: 'boolean'}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ####**Soft Limiter (Use 0.97 - 0.995 range):**\n",
        "#@markdown *Experimental! ...may help mitigate color clipping.*\n",
        "soft_limiter_on = False#@param{type: 'boolean'}\n",
        "soft_limiter_knee = .98 #@param{type: 'number'}\n",
        "if soft_limiter_knee < 0.5 or soft_limiter_knee > .999:\n",
        "  soft_limiter_knee = .98\n",
        "  print('soft_limiter_knee out of range. Automatically reset to 0.98')\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ####**Init Settings:**\n",
        "init_image = None #@param{type: 'string'}\n",
        "init_scale = 1000 #@param{type: 'integer'}\n",
        "skip_steps = 7 #@param{type: 'integer'}\n",
        "#@markdown *Make sure you set skip_steps to ~50% of your steps if you want to use an init image.*\n",
        "\n",
        "#@markdown ---\n",
        "skip_end_steps =  0#@param{type: 'integer'}\n",
        "#@markdown Will give an unfinished touch to your result, depending on how many end steps you want to skip.\n",
        "\n",
        "#Get corrected sizes\n",
        "side_x = (width_height[0]//64)*64;\n",
        "side_y = (width_height[1]//64)*64;\n",
        "if side_x != width_height[0] or side_y != width_height[1]:\n",
        "    print(f'Changing output size to {side_x}x{side_y}. Dimensions must by multiples of 64.')\n",
        "\n",
        "#Update Model Settings\n",
        "timestep_respacing = f'ddim{steps}'\n",
        "diffusion_steps = (1000//steps)*steps if steps < 1000 else steps\n",
        "model_config.update({\n",
        "    'timestep_respacing': timestep_respacing,\n",
        "    'diffusion_steps': diffusion_steps,\n",
        "})\n",
        "\n",
        "#Make folder for batch\n",
        "batchFolder = f'{outDirPath}/{batch_name}'\n",
        "createPath(batchFolder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnimSetTop"
      },
      "source": [
        "### Animation Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "cellView": "form",
        "id": "AnimSettings"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=====\n",
            "Turbo mode only available with 3D animations. Disabling Turbo.\n",
            "=====\n"
          ]
        }
      ],
      "source": [
        "#@markdown ####**Animation Mode:**\n",
        "animation_mode = 'None' #@param ['None', '2D', '3D', 'Video Input'] {type:'string'}\n",
        "#@markdown *For animation, you probably want to turn `cutn_batches` to 1 to make it quicker.*\n",
        "\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown ####**Video Input Settings:**\n",
        "if is_colab:\n",
        "    video_init_path = \"/content/training.mp4\" #@param {type: 'string'}\n",
        "else:\n",
        "    video_init_path = \"training.mp4\" #@param {type: 'string'}\n",
        "extract_nth_frame = 2 #@param {type: 'number'}\n",
        "video_init_seed_continuity = True #@param {type: 'boolean'}\n",
        "\n",
        "if animation_mode == \"Video Input\":\n",
        "  if is_colab:\n",
        "      videoFramesFolder = f'/content/videoFrames'\n",
        "  else:\n",
        "      videoFramesFolder = f'videoFrames'\n",
        "  createPath(videoFramesFolder)\n",
        "  print(f\"Exporting Video Frames (1 every {extract_nth_frame})...\")\n",
        "  try:\n",
        "      for f in pathlib.Path(f'{videoFramesFolder}').glob('*.jpg'):\n",
        "          f.unlink()\n",
        "  except:\n",
        "      print('')\n",
        "  vf = f'select=not(mod(n\\,{extract_nth_frame}))'\n",
        "  subprocess.run(['ffmpeg', '-i', f'{video_init_path}', '-vf', f'{vf}', '-vsync', 'vfr', '-q:v', '2', '-loglevel', 'error', '-stats', f'{videoFramesFolder}/%04d.jpg'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "  #!ffmpeg -i {video_init_path} -vf {vf} -vsync vfr -q:v 2 -loglevel error -stats {videoFramesFolder}/%04d.jpg\n",
        "\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown ####**2D Animation Settings:**\n",
        "#@markdown `zoom` is a multiplier of dimensions, 1 is no zoom.\n",
        "#@markdown All rotations are provided in degrees.\n",
        "\n",
        "key_frames = True #@param {type:\"boolean\"}\n",
        "max_frames = 10000#@param {type:\"number\"}\n",
        "\n",
        "if animation_mode == \"Video Input\":\n",
        "    max_frames = len(glob(f'{videoFramesFolder}/*.jpg'))\n",
        "\n",
        "interp_spline = 'Linear' #Do not change, currently will not look good. param ['Linear','Quadratic','Cubic']{type:\"string\"}\n",
        "angle = \"0:(0)\"#@param {type:\"string\"}\n",
        "zoom = \"0: (1.0), 10: (1.05)\"#@param {type:\"string\"}\n",
        "translation_x = \"0: (0)\"#@param {type:\"string\"}\n",
        "translation_y = \"0: (0)\"#@param {type:\"string\"}\n",
        "translation_z = \"0: (10.0)\"#@param {type:\"string\"}\n",
        "rotation_3d_x = \"0: (0)\"#@param {type:\"string\"}\n",
        "rotation_3d_y = \"0: (0)\"#@param {type:\"string\"}\n",
        "rotation_3d_z = \"0: (0)\"#@param {type:\"string\"}\n",
        "midas_depth_model = \"dpt_large\"#@param {type:\"string\"}\n",
        "midas_weight = 0.3#@param {type:\"number\"}\n",
        "near_plane = 100#@param {type:\"number\"}\n",
        "far_plane = 10000#@param {type:\"number\"}\n",
        "fov = 120#@param {type:\"number\"}\n",
        "padding_mode = 'border'#@param {type:\"string\"}\n",
        "sampling_mode = 'bicubic'#@param {type:\"string\"}\n",
        "\n",
        "#======= TURBO MODE\n",
        "#@markdown ---\n",
        "#@markdown ####**Turbo Mode (3D anim only):**\n",
        "#@markdown (Starts after frame 10,) skips diffusion steps and just uses depth map to warp images for skipped frames.\n",
        "#@markdown Speeds up rendering by 2x-4x, and may improve image coherence between frames. frame_blend_mode smooths abrupt texture changes across 2 frames.\n",
        "#@markdown For different settings tuned for Turbo Mode, refer to the original Disco-Turbo Github: https://github.com/zippy731/disco-diffusion-turbo\n",
        "\n",
        "turbo_mode = True #@param {type:\"boolean\"}\n",
        "turbo_steps = \"3\" #@param [\"2\",\"3\",\"4\",\"5\",\"6\"] {type:\"string\"}\n",
        "turbo_preroll = 10 # frames\n",
        "\n",
        "#insist turbo be used only w 3d anim.\n",
        "if turbo_mode and animation_mode != '3D':\n",
        "    print('=====')\n",
        "    print('Turbo mode only available with 3D animations. Disabling Turbo.')\n",
        "    print('=====')\n",
        "    turbo_mode = False\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown ####**Coherency Settings:**\n",
        "#@markdown `frame_scale` tries to guide the new frame to looking like the old one. A good default is 1500.\n",
        "frames_scale = 0 #@param{type: 'integer'}\n",
        "#@markdown `frame_skip_steps` will blur the previous frame - higher values will flicker less but struggle to add enough new detail to zoom into.\n",
        "frames_skip_steps = '70%' #@param ['40%', '50%', '60%', '65%', '70%', '75%','80%'] {type: 'string'}\n",
        "#@markdown `frames_skip_end_steps` will skip the tail end of the diffusion - higher values will leave final frame with slightly less detail.\n",
        "frames_skip_end_steps = 0 #@param{type: 'integer'}\n",
        "\n",
        "#============= NOISE INJECTOR\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ####**Noise Injector (3D anim only):**\n",
        "#@markdown (experimental!) Adds noise if dead spots detected\n",
        "noise_injector = False #@param {type:\"boolean\"}\n",
        "noise_injector_threshold =  50#@param{type: 'integer'}\n",
        "#@markdown `noise_injector_threshold` if edge% falls below this %, will boost next frame. 20 is a good starting value.\n",
        "noise_injector_mix = 0.10 #@param{type: 'number'}\n",
        "#@markdown `noise_injector_mix` overall wet/dry mix for noise effect 0 - 1.0 \n",
        "noise_injector_px_size =  1#@param{type: 'integer'}\n",
        "#@markdown `noise_injector_px_size` size of noise color blocks.\n",
        "noise_injector_spice = .10 #@param{type: 'number'}\n",
        "#@markdown `noise_injector_spice` adds random noise to color-matched noise. 0 - 1.0 ratio.\n",
        "noise_injector_mask_feather = 10 #@param{type: 'integer'}\n",
        "#@markdown `noise_injector_mask_feather` introduce soft edges at noise mask distance in px. \n",
        "noise_injector_sigma = 32 #@param{type: 'integer'}\n",
        "#@markdown `noise_injector_sigma` = size of 'clumps' in noise. higher is larger, 16-32 are good starting pt \n",
        "\n",
        "#@markdown ####**Copy palette conform animation frames to color mix from palette_image (3D anim /noise inj only):**\n",
        "copy_palette = False#@param{type: 'boolean'}\n",
        "copy_palette_image = \"\" #@param{type: 'string'}\n",
        "\n",
        "\n",
        "#======= VR MODE\n",
        "#@markdown ---\n",
        "#@markdown ####**VR Mode (3D anim only):**\n",
        "#@markdown Enables stereo rendering of left/right eye views (supporting Turbo) which use a different (fish-eye) camera projection matrix.   \n",
        "#@markdown Note the images you're prompting will work better if they have some inherent wide-angle aspect\n",
        "#@markdown The generated images will need to be combined into left/right videos. These can then be stitched into the VR180 format.\n",
        "#@markdown Google made the VR180 Creator tool but subsequently stopped supporting it. It's available for download in a few places including https://www.patrickgrunwald.de/vr180-creator-download\n",
        "#@markdown The tool is not only good for stitching (videos and photos) but also for adding the correct metadata into existing videos, which is needed for services like YouTube to identify the format correctly.\n",
        "#@markdown Watching YouTube VR videos isn't necessarily the easiest depending on your headset. For instance Oculus have a dedicated media studio and store which makes the files easier to access on a Quest https://creator.oculus.com/manage/mediastudio/\n",
        "#@markdown \n",
        "#@markdown The command to get ffmpeg to concat your frames for each eye is in the form: `ffmpeg -framerate 15 -i frame_%4d_l.png l.mp4` (repeat for r)\n",
        "\n",
        "vr_mode = False #@param {type:\"boolean\"}\n",
        "#@markdown `vr_eye_angle` is the y-axis rotation of the eyes towards the center\n",
        "vr_eye_angle = 0.5 #@param{type:\"number\"}\n",
        "#@markdown interpupillary distance (between the eyes)\n",
        "vr_ipd = 5.0 #@param{type:\"number\"}\n",
        "\n",
        "#insist VR be used only w 3d anim.\n",
        "if vr_mode and animation_mode != '3D':\n",
        "    print('=====')\n",
        "    print('VR mode only available with 3D animations. Disabling VR.')\n",
        "    print('=====')\n",
        "    vr_mode = False\n",
        "\n",
        "\n",
        "def parse_key_frames(string, prompt_parser=None):\n",
        "    \"\"\"Given a string representing frame numbers paired with parameter values at that frame,\n",
        "    return a dictionary with the frame numbers as keys and the parameter values as the values.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    string: string\n",
        "        Frame numbers paired with parameter values at that frame number, in the format\n",
        "        'framenumber1: (parametervalues1), framenumber2: (parametervalues2), ...'\n",
        "    prompt_parser: function or None, optional\n",
        "        If provided, prompt_parser will be applied to each string of parameter values.\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    dict\n",
        "        Frame numbers as keys, parameter values at that frame number as values\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    RuntimeError\n",
        "        If the input string does not match the expected format.\n",
        "    \n",
        "    Examples\n",
        "    --------\n",
        "    >>> parse_key_frames(\"10:(Apple: 1| Orange: 0), 20: (Apple: 0| Orange: 1| Peach: 1)\")\n",
        "    {10: 'Apple: 1| Orange: 0', 20: 'Apple: 0| Orange: 1| Peach: 1'}\n",
        "\n",
        "    >>> parse_key_frames(\"10:(Apple: 1| Orange: 0), 20: (Apple: 0| Orange: 1| Peach: 1)\", prompt_parser=lambda x: x.lower()))\n",
        "    {10: 'apple: 1| orange: 0', 20: 'apple: 0| orange: 1| peach: 1'}\n",
        "    \"\"\"\n",
        "    import re\n",
        "    pattern = r'((?P<frame>[0-9]+):[\\s]*[\\(](?P<param>[\\S\\s]*?)[\\)])'\n",
        "    frames = dict()\n",
        "    for match_object in re.finditer(pattern, string):\n",
        "        frame = int(match_object.groupdict()['frame'])\n",
        "        param = match_object.groupdict()['param']\n",
        "        if prompt_parser:\n",
        "            frames[frame] = prompt_parser(param)\n",
        "        else:\n",
        "            frames[frame] = param\n",
        "\n",
        "    if frames == {} and len(string) != 0:\n",
        "        raise RuntimeError('Key Frame string not correctly formatted')\n",
        "    return frames\n",
        "\n",
        "def get_inbetweens(key_frames, integer=False):\n",
        "    \"\"\"Given a dict with frame numbers as keys and a parameter value as values,\n",
        "    return a pandas Series containing the value of the parameter at every frame from 0 to max_frames.\n",
        "    Any values not provided in the input dict are calculated by linear interpolation between\n",
        "    the values of the previous and next provided frames. If there is no previous provided frame, then\n",
        "    the value is equal to the value of the next provided frame, or if there is no next provided frame,\n",
        "    then the value is equal to the value of the previous provided frame. If no frames are provided,\n",
        "    all frame values are NaN.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    key_frames: dict\n",
        "        A dict with integer frame numbers as keys and numerical values of a particular parameter as values.\n",
        "    integer: Bool, optional\n",
        "        If True, the values of the output series are converted to integers.\n",
        "        Otherwise, the values are floats.\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    pd.Series\n",
        "        A Series with length max_frames representing the parameter values for each frame.\n",
        "    \n",
        "    Examples\n",
        "    --------\n",
        "    >>> max_frames = 5\n",
        "    >>> get_inbetweens({1: 5, 3: 6})\n",
        "    0    5.0\n",
        "    1    5.0\n",
        "    2    5.5\n",
        "    3    6.0\n",
        "    4    6.0\n",
        "    dtype: float64\n",
        "\n",
        "    >>> get_inbetweens({1: 5, 3: 6}, integer=True)\n",
        "    0    5\n",
        "    1    5\n",
        "    2    5\n",
        "    3    6\n",
        "    4    6\n",
        "    dtype: int64\n",
        "    \"\"\"\n",
        "    key_frame_series = pd.Series([np.nan for a in range(max_frames)])\n",
        "\n",
        "    for i, value in key_frames.items():\n",
        "        key_frame_series[i] = value\n",
        "    key_frame_series = key_frame_series.astype(float)\n",
        "    \n",
        "    interp_method = interp_spline\n",
        "\n",
        "    if interp_method == 'Cubic' and len(key_frames.items()) <=3:\n",
        "      interp_method = 'Quadratic'\n",
        "    \n",
        "    if interp_method == 'Quadratic' and len(key_frames.items()) <= 2:\n",
        "      interp_method = 'Linear'\n",
        "      \n",
        "    \n",
        "    key_frame_series[0] = key_frame_series[key_frame_series.first_valid_index()]\n",
        "    key_frame_series[max_frames-1] = key_frame_series[key_frame_series.last_valid_index()]\n",
        "    # key_frame_series = key_frame_series.interpolate(method=intrp_method,order=1, limit_direction='both')\n",
        "    key_frame_series = key_frame_series.interpolate(method=interp_method.lower(),limit_direction='both')\n",
        "    if integer:\n",
        "        return key_frame_series.astype(int)\n",
        "    return key_frame_series\n",
        "\n",
        "def split_prompts(prompts):\n",
        "    prompt_series = pd.Series([np.nan for a in range(max_frames)])\n",
        "    for i, prompt in prompts.items():\n",
        "        prompt_series[i] = prompt\n",
        "    # prompt_series = prompt_series.astype(str)\n",
        "    prompt_series = prompt_series.ffill().bfill()\n",
        "    return prompt_series\n",
        "\n",
        "if key_frames:\n",
        "    try:\n",
        "        angle_series = get_inbetweens(parse_key_frames(angle))\n",
        "    except RuntimeError as e:\n",
        "        print(\n",
        "            \"WARNING: You have selected to use key frames, but you have not \"\n",
        "            \"formatted `angle` correctly for key frames.\\n\"\n",
        "            \"Attempting to interpret `angle` as \"\n",
        "            f'\"0: ({angle})\"\\n'\n",
        "            \"Please read the instructions to find out how to use key frames \"\n",
        "            \"correctly.\\n\"\n",
        "        )\n",
        "        angle = f\"0: ({angle})\"\n",
        "        angle_series = get_inbetweens(parse_key_frames(angle))\n",
        "\n",
        "    try:\n",
        "        zoom_series = get_inbetweens(parse_key_frames(zoom))\n",
        "    except RuntimeError as e:\n",
        "        print(\n",
        "            \"WARNING: You have selected to use key frames, but you have not \"\n",
        "            \"formatted `zoom` correctly for key frames.\\n\"\n",
        "            \"Attempting to interpret `zoom` as \"\n",
        "            f'\"0: ({zoom})\"\\n'\n",
        "            \"Please read the instructions to find out how to use key frames \"\n",
        "            \"correctly.\\n\"\n",
        "        )\n",
        "        zoom = f\"0: ({zoom})\"\n",
        "        zoom_series = get_inbetweens(parse_key_frames(zoom))\n",
        "\n",
        "    try:\n",
        "        translation_x_series = get_inbetweens(parse_key_frames(translation_x))\n",
        "    except RuntimeError as e:\n",
        "        print(\n",
        "            \"WARNING: You have selected to use key frames, but you have not \"\n",
        "            \"formatted `translation_x` correctly for key frames.\\n\"\n",
        "            \"Attempting to interpret `translation_x` as \"\n",
        "            f'\"0: ({translation_x})\"\\n'\n",
        "            \"Please read the instructions to find out how to use key frames \"\n",
        "            \"correctly.\\n\"\n",
        "        )\n",
        "        translation_x = f\"0: ({translation_x})\"\n",
        "        translation_x_series = get_inbetweens(parse_key_frames(translation_x))\n",
        "\n",
        "    try:\n",
        "        translation_y_series = get_inbetweens(parse_key_frames(translation_y))\n",
        "    except RuntimeError as e:\n",
        "        print(\n",
        "            \"WARNING: You have selected to use key frames, but you have not \"\n",
        "            \"formatted `translation_y` correctly for key frames.\\n\"\n",
        "            \"Attempting to interpret `translation_y` as \"\n",
        "            f'\"0: ({translation_y})\"\\n'\n",
        "            \"Please read the instructions to find out how to use key frames \"\n",
        "            \"correctly.\\n\"\n",
        "        )\n",
        "        translation_y = f\"0: ({translation_y})\"\n",
        "        translation_y_series = get_inbetweens(parse_key_frames(translation_y))\n",
        "\n",
        "    try:\n",
        "        translation_z_series = get_inbetweens(parse_key_frames(translation_z))\n",
        "    except RuntimeError as e:\n",
        "        print(\n",
        "            \"WARNING: You have selected to use key frames, but you have not \"\n",
        "            \"formatted `translation_z` correctly for key frames.\\n\"\n",
        "            \"Attempting to interpret `translation_z` as \"\n",
        "            f'\"0: ({translation_z})\"\\n'\n",
        "            \"Please read the instructions to find out how to use key frames \"\n",
        "            \"correctly.\\n\"\n",
        "        )\n",
        "        translation_z = f\"0: ({translation_z})\"\n",
        "        translation_z_series = get_inbetweens(parse_key_frames(translation_z))\n",
        "\n",
        "    try:\n",
        "        rotation_3d_x_series = get_inbetweens(parse_key_frames(rotation_3d_x))\n",
        "    except RuntimeError as e:\n",
        "        print(\n",
        "            \"WARNING: You have selected to use key frames, but you have not \"\n",
        "            \"formatted `rotation_3d_x` correctly for key frames.\\n\"\n",
        "            \"Attempting to interpret `rotation_3d_x` as \"\n",
        "            f'\"0: ({rotation_3d_x})\"\\n'\n",
        "            \"Please read the instructions to find out how to use key frames \"\n",
        "            \"correctly.\\n\"\n",
        "        )\n",
        "        rotation_3d_x = f\"0: ({rotation_3d_x})\"\n",
        "        rotation_3d_x_series = get_inbetweens(parse_key_frames(rotation_3d_x))\n",
        "\n",
        "    try:\n",
        "        rotation_3d_y_series = get_inbetweens(parse_key_frames(rotation_3d_y))\n",
        "    except RuntimeError as e:\n",
        "        print(\n",
        "            \"WARNING: You have selected to use key frames, but you have not \"\n",
        "            \"formatted `rotation_3d_y` correctly for key frames.\\n\"\n",
        "            \"Attempting to interpret `rotation_3d_y` as \"\n",
        "            f'\"0: ({rotation_3d_y})\"\\n'\n",
        "            \"Please read the instructions to find out how to use key frames \"\n",
        "            \"correctly.\\n\"\n",
        "        )\n",
        "        rotation_3d_y = f\"0: ({rotation_3d_y})\"\n",
        "        rotation_3d_y_series = get_inbetweens(parse_key_frames(rotation_3d_y))\n",
        "\n",
        "    try:\n",
        "        rotation_3d_z_series = get_inbetweens(parse_key_frames(rotation_3d_z))\n",
        "    except RuntimeError as e:\n",
        "        print(\n",
        "            \"WARNING: You have selected to use key frames, but you have not \"\n",
        "            \"formatted `rotation_3d_z` correctly for key frames.\\n\"\n",
        "            \"Attempting to interpret `rotation_3d_z` as \"\n",
        "            f'\"0: ({rotation_3d_z})\"\\n'\n",
        "            \"Please read the instructions to find out how to use key frames \"\n",
        "            \"correctly.\\n\"\n",
        "        )\n",
        "        rotation_3d_z = f\"0: ({rotation_3d_z})\"\n",
        "        rotation_3d_z_series = get_inbetweens(parse_key_frames(rotation_3d_z))\n",
        "\n",
        "else:\n",
        "    angle = float(angle)\n",
        "    zoom = float(zoom)\n",
        "    translation_x = float(translation_x)\n",
        "    translation_y = float(translation_y)\n",
        "    translation_z = float(translation_z)\n",
        "    rotation_3d_x = float(rotation_3d_x)\n",
        "    rotation_3d_y = float(rotation_3d_y)\n",
        "    rotation_3d_z = float(rotation_3d_z)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExtraSetTop"
      },
      "source": [
        "\n",
        "### Extra Settings\n",
        " Partial Saves, Advanced Settings, Cutn Scheduling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "cellView": "form",
        "id": "ExtraSettings"
      },
      "outputs": [],
      "source": [
        "#@markdown ####**Saving:**\n",
        "\n",
        "intermediate_saves = 0#@param{type: 'raw'}\n",
        "intermediates_in_subfolder = True #@param{type: 'boolean'}\n",
        "#@markdown Intermediate steps will save a copy at your specified intervals. You can either format it as a single integer or a list of specific steps \n",
        "\n",
        "#@markdown A value of `2` will save a copy at 33% and 66%. 0 will save none.\n",
        "\n",
        "#@markdown A value of `[5, 9, 34, 45]` will save at steps 5, 9, 34, and 45. (Make sure to include the brackets)\n",
        "\n",
        "\n",
        "if type(intermediate_saves) is not list:\n",
        "    if intermediate_saves:\n",
        "        steps_per_checkpoint = math.floor((steps - skip_steps - 1) // (intermediate_saves+1))\n",
        "        steps_per_checkpoint = steps_per_checkpoint if steps_per_checkpoint > 0 else 1\n",
        "        print(f'Will save every {steps_per_checkpoint} steps')\n",
        "    else:\n",
        "        steps_per_checkpoint = steps+10\n",
        "else:\n",
        "    steps_per_checkpoint = None\n",
        "\n",
        "if intermediate_saves and intermediates_in_subfolder is True:\n",
        "    partialFolder = f'{batchFolder}/partials'\n",
        "    createPath(partialFolder)\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown ####**Advanced Settings:**\n",
        "#@markdown *There are a few extra advanced settings available if you double click this cell.*\n",
        "\n",
        "#@markdown *Perlin init will replace your init, so uncheck if using one.*\n",
        "\n",
        "perlin_init = False  #@param{type: 'boolean'}\n",
        "perlin_mode = 'mixed' #@param ['mixed', 'color', 'gray']\n",
        "set_seed = 'random_seed' #@param{type: 'string'}\n",
        "eta = .75#@param{type: 'number'}\n",
        "clamp_grad = True #@param{type: 'boolean'}\n",
        "clamp_max = 0.05 #@param{type: 'number'}\n",
        "\n",
        "\n",
        "### EXTRA ADVANCED SETTINGS:\n",
        "randomize_class = True\n",
        "clip_denoised = False\n",
        "fuzzy_prompt = False\n",
        "rand_mag = 0.05\n",
        "\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown ####**Cutn Scheduling:**\n",
        "#@markdown Format: `[40]*400+[20]*600` = 40 cuts for the first 400 /1000 steps, then 20 for the last 600/1000\n",
        "\n",
        "#@markdown cut_overview and cut_innercut are cumulative for total cutn on any given step. Overview cuts see the entire image and are good for early structure, innercuts are your standard cutn.\n",
        "\n",
        "cut_overview = \"[25]*400+[2]*600\" #@param {type: 'string'}       \n",
        "cut_innercut =\"[4]*400+[25]*600\"#@param {type: 'string'}  \n",
        "\n",
        "#@markdown For Pixel Art Diffusion, [cut_ic_pow](https://ezcharts.miraheze.org/wiki/Category:Cut_ic_pow) values seem to represent pixel density. Values between 1 and 50000 all work, and it's likely that even higher values do, as well. Values can be expressed as decimal numbers, not just integers. Check the Readme at the top of the notebook or click [here](https://i.imgur.com/lAb2HU1.jpeg) for some examples. Note how  the lowest values are most person-like and the highest values become much more architectural. Also note the shift from cooler colors at the top left to warmer colors at the bottom right.\n",
        "\n",
        "cut_ic_pow = 1000#@param {type: 'number'}  \n",
        "cut_icgray_p = \"[0.2]*400+[0]*600\"#@param {type: 'string'}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PromptsTop"
      },
      "source": [
        "### Prompts\n",
        "`animation_mode: None` will only use the first set. `animation_mode: 2D / Video` will run through them per the set frames and hold on the last one.\n",
        "\n",
        "------\n",
        "\n",
        "#### Pixel Art Diffusion Prompts\n",
        "Although all PAD models are fine-tuned on pixel art, PAD still may need gentle reminders that you *do* want pixel art. This is especially true when using the Hard or Soft PAD models (though possibly not with Secondary enabled). \n",
        "\n",
        "Lack of pixelation isn't really an issue with simple prompts in PAD 4k, but longer/more complex prompts may benefit from extra nudging. When needing a little extra pixellation, a good rule of thumb for prompt structuring is to follow a format like: \n",
        "\n",
        "```\n",
        "[\"A cyberpunk city at sunset, #pixelart by van gogh\",\"#pixelart\"]\n",
        "```\n",
        "With a multi-part prompt, try playing around with how many times you remind it to generate images in a pixel art style. For examples of prmpt structure given varying length of prompts, see the README at the top of the page! \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Prompts"
      },
      "outputs": [],
      "source": [
        "text_prompts = {\n",
        "    0: [\"A synthwave city at sunset by van gogh\",\"#pixelart\"],\n",
        "    #100: [\"This set of prompts start at frame 100\",\"This prompt has weight five:5\"],\n",
        "}\n",
        "\n",
        "image_prompts = {\n",
        "    # 0:['ImagePromptsWorkButArentVeryGood.png:2',],\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiffuseTop"
      },
      "source": [
        "# 4. Diffuse!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "cellView": "form",
        "id": "DoTheRun"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d347c6c2657840c886e162790f3ecf1c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/9 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "65562f3b19ba4a3584f4292bf04b47f6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Output()"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "93bbd31516c04664ba69076ed58f347c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/68 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Seed used: 1181091160\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "CUDA out of memory. Tried to allocate 60.00 MiB (GPU 0; 10.76 GiB total capacity; 8.61 GiB already allocated; 60.00 MiB free; 9.33 GiB reserved in total by PyTorch)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32m/disco-diffusion/Pixel_Art_Diffusion_v2_01.ipynb Cell 25'\u001b[0m in \u001b[0;36m<cell line: 186>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f67726163696f75735f686579726f76736b79222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6963755f677075227d7d/disco-diffusion/Pixel_Art_Diffusion_v2_01.ipynb#ch0000024vscode-remote?line=184'>185</a>\u001b[0m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mempty_cache()\n\u001b[1;32m    <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f67726163696f75735f686579726f76736b79222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6963755f677075227d7d/disco-diffusion/Pixel_Art_Diffusion_v2_01.ipynb#ch0000024vscode-remote?line=185'>186</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f67726163696f75735f686579726f76736b79222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6963755f677075227d7d/disco-diffusion/Pixel_Art_Diffusion_v2_01.ipynb#ch0000024vscode-remote?line=186'>187</a>\u001b[0m     do_run()\n\u001b[1;32m    <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f67726163696f75735f686579726f76736b79222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6963755f677075227d7d/disco-diffusion/Pixel_Art_Diffusion_v2_01.ipynb#ch0000024vscode-remote?line=187'>188</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m    <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f67726163696f75735f686579726f76736b79222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6963755f677075227d7d/disco-diffusion/Pixel_Art_Diffusion_v2_01.ipynb#ch0000024vscode-remote?line=188'>189</a>\u001b[0m     \u001b[39mpass\u001b[39;00m\n",
            "\u001b[1;32m/disco-diffusion/Pixel_Art_Diffusion_v2_01.ipynb Cell 12'\u001b[0m in \u001b[0;36mdo_run\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f67726163696f75735f686579726f76736b79222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6963755f677075227d7d/disco-diffusion/Pixel_Art_Diffusion_v2_01.ipynb#ch0000011vscode-remote?line=819'>820</a>\u001b[0m     samples \u001b[39m=\u001b[39m sample_fn(\n\u001b[1;32m    <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f67726163696f75735f686579726f76736b79222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6963755f677075227d7d/disco-diffusion/Pixel_Art_Diffusion_v2_01.ipynb#ch0000011vscode-remote?line=820'>821</a>\u001b[0m         model,\n\u001b[1;32m    <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f67726163696f75735f686579726f76736b79222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6963755f677075227d7d/disco-diffusion/Pixel_Art_Diffusion_v2_01.ipynb#ch0000011vscode-remote?line=821'>822</a>\u001b[0m         (batch_size, \u001b[39m3\u001b[39m, args\u001b[39m.\u001b[39mside_y, args\u001b[39m.\u001b[39mside_x),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f67726163696f75735f686579726f76736b79222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6963755f677075227d7d/disco-diffusion/Pixel_Art_Diffusion_v2_01.ipynb#ch0000011vscode-remote?line=829'>830</a>\u001b[0m         order\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[1;32m    <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f67726163696f75735f686579726f76736b79222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6963755f677075227d7d/disco-diffusion/Pixel_Art_Diffusion_v2_01.ipynb#ch0000011vscode-remote?line=830'>831</a>\u001b[0m     )\n\u001b[1;32m    <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f67726163696f75735f686579726f76736b79222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6963755f677075227d7d/disco-diffusion/Pixel_Art_Diffusion_v2_01.ipynb#ch0000011vscode-remote?line=833'>834</a>\u001b[0m \u001b[39m# with run_display:\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f67726163696f75735f686579726f76736b79222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6963755f677075227d7d/disco-diffusion/Pixel_Art_Diffusion_v2_01.ipynb#ch0000011vscode-remote?line=834'>835</a>\u001b[0m \u001b[39m# display.clear_output(wait=True)\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f67726163696f75735f686579726f76736b79222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6963755f677075227d7d/disco-diffusion/Pixel_Art_Diffusion_v2_01.ipynb#ch0000011vscode-remote?line=835'>836</a>\u001b[0m \u001b[39mfor\u001b[39;00m j, sample \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(samples):    \n\u001b[1;32m    <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f67726163696f75735f686579726f76736b79222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6963755f677075227d7d/disco-diffusion/Pixel_Art_Diffusion_v2_01.ipynb#ch0000011vscode-remote?line=836'>837</a>\u001b[0m   cur_t \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f67726163696f75735f686579726f76736b79222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6963755f677075227d7d/disco-diffusion/Pixel_Art_Diffusion_v2_01.ipynb#ch0000011vscode-remote?line=837'>838</a>\u001b[0m   intermediateStep \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
            "File \u001b[0;32m/disco-diffusion/guided-diffusion/guided_diffusion/gaussian_diffusion.py:897\u001b[0m, in \u001b[0;36mGaussianDiffusion.ddim_sample_loop_progressive\u001b[0;34m(self, model, shape, noise, clip_denoised, denoised_fn, cond_fn, model_kwargs, device, progress, eta, skip_timesteps, init_image, randomize_class, cond_fn_with_grad, transformation_fn, transformation_percent)\u001b[0m\n\u001b[1;32m    895\u001b[0m   img \u001b[39m=\u001b[39m transformation_fn(img)\n\u001b[1;32m    896\u001b[0m sample_fn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mddim_sample_with_grad \u001b[39mif\u001b[39;00m cond_fn_with_grad \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mddim_sample\n\u001b[0;32m--> 897\u001b[0m out \u001b[39m=\u001b[39m sample_fn(\n\u001b[1;32m    898\u001b[0m     model,\n\u001b[1;32m    899\u001b[0m     img,\n\u001b[1;32m    900\u001b[0m     t,\n\u001b[1;32m    901\u001b[0m     clip_denoised\u001b[39m=\u001b[39;49mclip_denoised,\n\u001b[1;32m    902\u001b[0m     denoised_fn\u001b[39m=\u001b[39;49mdenoised_fn,\n\u001b[1;32m    903\u001b[0m     cond_fn\u001b[39m=\u001b[39;49mcond_fn,\n\u001b[1;32m    904\u001b[0m     model_kwargs\u001b[39m=\u001b[39;49mmodel_kwargs,\n\u001b[1;32m    905\u001b[0m     eta\u001b[39m=\u001b[39;49meta,\n\u001b[1;32m    906\u001b[0m )\n\u001b[1;32m    907\u001b[0m \u001b[39myield\u001b[39;00m out\n\u001b[1;32m    908\u001b[0m img \u001b[39m=\u001b[39m out[\u001b[39m\"\u001b[39m\u001b[39msample\u001b[39m\u001b[39m\"\u001b[39m]\n",
            "File \u001b[0;32m/disco-diffusion/guided-diffusion/guided_diffusion/gaussian_diffusion.py:674\u001b[0m, in \u001b[0;36mGaussianDiffusion.ddim_sample\u001b[0;34m(self, model, x, t, clip_denoised, denoised_fn, cond_fn, model_kwargs, eta)\u001b[0m\n\u001b[1;32m    665\u001b[0m out_orig \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mp_mean_variance(\n\u001b[1;32m    666\u001b[0m     model,\n\u001b[1;32m    667\u001b[0m     x,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    671\u001b[0m     model_kwargs\u001b[39m=\u001b[39mmodel_kwargs,\n\u001b[1;32m    672\u001b[0m )\n\u001b[1;32m    673\u001b[0m \u001b[39mif\u001b[39;00m cond_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 674\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcondition_score(cond_fn, out_orig, x, t, model_kwargs\u001b[39m=\u001b[39;49mmodel_kwargs)\n\u001b[1;32m    675\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    676\u001b[0m     out \u001b[39m=\u001b[39m out_orig\n",
            "File \u001b[0;32m/disco-diffusion/guided-diffusion/guided_diffusion/respace.py:102\u001b[0m, in \u001b[0;36mSpacedDiffusion.condition_score\u001b[0;34m(self, cond_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcondition_score\u001b[39m(\u001b[39mself\u001b[39m, cond_fn, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 102\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcondition_score(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_wrap_model(cond_fn), \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m/disco-diffusion/guided-diffusion/guided_diffusion/gaussian_diffusion.py:399\u001b[0m, in \u001b[0;36mGaussianDiffusion.condition_score\u001b[0;34m(self, cond_fn, p_mean_var, x, t, model_kwargs)\u001b[0m\n\u001b[1;32m    396\u001b[0m alpha_bar \u001b[39m=\u001b[39m _extract_into_tensor(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39malphas_cumprod, t, x\u001b[39m.\u001b[39mshape)\n\u001b[1;32m    398\u001b[0m eps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_predict_eps_from_xstart(x, t, p_mean_var[\u001b[39m\"\u001b[39m\u001b[39mpred_xstart\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m--> 399\u001b[0m eps \u001b[39m=\u001b[39m eps \u001b[39m-\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m alpha_bar)\u001b[39m.\u001b[39msqrt() \u001b[39m*\u001b[39m cond_fn(\n\u001b[1;32m    400\u001b[0m     x, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_scale_timesteps(t), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs\n\u001b[1;32m    401\u001b[0m )\n\u001b[1;32m    403\u001b[0m out \u001b[39m=\u001b[39m p_mean_var\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m    404\u001b[0m out[\u001b[39m\"\u001b[39m\u001b[39mpred_xstart\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_predict_xstart_from_eps(x, t, eps)\n",
            "File \u001b[0;32m/disco-diffusion/guided-diffusion/guided_diffusion/respace.py:128\u001b[0m, in \u001b[0;36m_WrappedModel.__call__\u001b[0;34m(self, x, ts, **kwargs)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrescale_timesteps:\n\u001b[1;32m    127\u001b[0m     new_ts \u001b[39m=\u001b[39m new_ts\u001b[39m.\u001b[39mfloat() \u001b[39m*\u001b[39m (\u001b[39m1000.0\u001b[39m \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moriginal_num_steps)\n\u001b[0;32m--> 128\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(x, new_ts, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "\u001b[1;32m/disco-diffusion/Pixel_Art_Diffusion_v2_01.ipynb Cell 12'\u001b[0m in \u001b[0;36mdo_run.<locals>.cond_fn\u001b[0;34m(x, t, y)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f67726163696f75735f686579726f76736b79222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6963755f677075227d7d/disco-diffusion/Pixel_Art_Diffusion_v2_01.ipynb#ch0000011vscode-remote?line=743'>744</a>\u001b[0m cuts \u001b[39m=\u001b[39m MakeCutoutsDango(input_resolution,\n\u001b[1;32m    <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f67726163696f75735f686579726f76736b79222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6963755f677075227d7d/disco-diffusion/Pixel_Art_Diffusion_v2_01.ipynb#ch0000011vscode-remote?line=744'>745</a>\u001b[0m         Overview\u001b[39m=\u001b[39m args\u001b[39m.\u001b[39mcut_overview[\u001b[39m1000\u001b[39m\u001b[39m-\u001b[39mt_int], \n\u001b[1;32m    <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f67726163696f75735f686579726f76736b79222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6963755f677075227d7d/disco-diffusion/Pixel_Art_Diffusion_v2_01.ipynb#ch0000011vscode-remote?line=745'>746</a>\u001b[0m         InnerCrop \u001b[39m=\u001b[39m args\u001b[39m.\u001b[39mcut_innercut[\u001b[39m1000\u001b[39m\u001b[39m-\u001b[39mt_int], IC_Size_Pow\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39mcut_ic_pow, IC_Grey_P \u001b[39m=\u001b[39m args\u001b[39m.\u001b[39mcut_icgray_p[\u001b[39m1000\u001b[39m\u001b[39m-\u001b[39mt_int]\n\u001b[1;32m    <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f67726163696f75735f686579726f76736b79222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6963755f677075227d7d/disco-diffusion/Pixel_Art_Diffusion_v2_01.ipynb#ch0000011vscode-remote?line=746'>747</a>\u001b[0m         )\n\u001b[1;32m    <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f67726163696f75735f686579726f76736b79222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6963755f677075227d7d/disco-diffusion/Pixel_Art_Diffusion_v2_01.ipynb#ch0000011vscode-remote?line=747'>748</a>\u001b[0m clip_in \u001b[39m=\u001b[39m normalize(cuts(x_in\u001b[39m.\u001b[39madd(\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mdiv(\u001b[39m2\u001b[39m)))\n\u001b[0;32m--> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f67726163696f75735f686579726f76736b79222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6963755f677075227d7d/disco-diffusion/Pixel_Art_Diffusion_v2_01.ipynb#ch0000011vscode-remote?line=748'>749</a>\u001b[0m image_embeds \u001b[39m=\u001b[39m model_stat[\u001b[39m\"\u001b[39;49m\u001b[39mclip_model\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mencode_image(clip_in)\u001b[39m.\u001b[39mfloat()\n\u001b[1;32m    <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f67726163696f75735f686579726f76736b79222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6963755f677075227d7d/disco-diffusion/Pixel_Art_Diffusion_v2_01.ipynb#ch0000011vscode-remote?line=749'>750</a>\u001b[0m dists \u001b[39m=\u001b[39m spherical_dist_loss(image_embeds\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m), model_stat[\u001b[39m\"\u001b[39m\u001b[39mtarget_embeds\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m))\n\u001b[1;32m    <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f67726163696f75735f686579726f76736b79222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6963755f677075227d7d/disco-diffusion/Pixel_Art_Diffusion_v2_01.ipynb#ch0000011vscode-remote?line=750'>751</a>\u001b[0m dists \u001b[39m=\u001b[39m dists\u001b[39m.\u001b[39mview([args\u001b[39m.\u001b[39mcut_overview[\u001b[39m1000\u001b[39m\u001b[39m-\u001b[39mt_int]\u001b[39m+\u001b[39margs\u001b[39m.\u001b[39mcut_innercut[\u001b[39m1000\u001b[39m\u001b[39m-\u001b[39mt_int], n, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n",
            "File \u001b[0;32m/disco-diffusion/CLIP/clip/model.py:342\u001b[0m, in \u001b[0;36mCLIP.encode_image\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mencode_image\u001b[39m(\u001b[39mself\u001b[39m, image):\n\u001b[0;32m--> 342\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvisual(image\u001b[39m.\u001b[39;49mtype(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdtype))\n",
            "File \u001b[0;32m~/miniconda3/envs/diffusion/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m/disco-diffusion/CLIP/clip/model.py:233\u001b[0m, in \u001b[0;36mVisionTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    230\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_pre(x)\n\u001b[1;32m    232\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m)  \u001b[39m# NLD -> LND\u001b[39;00m\n\u001b[0;32m--> 233\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(x)\n\u001b[1;32m    234\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m)  \u001b[39m# LND -> NLD\u001b[39;00m\n\u001b[1;32m    236\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_post(x[:, \u001b[39m0\u001b[39m, :])\n",
            "File \u001b[0;32m~/miniconda3/envs/diffusion/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m/disco-diffusion/CLIP/clip/model.py:204\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m--> 204\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mresblocks(x)\n",
            "File \u001b[0;32m~/miniconda3/envs/diffusion/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/miniconda3/envs/diffusion/lib/python3.9/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
            "File \u001b[0;32m~/miniconda3/envs/diffusion/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m/disco-diffusion/CLIP/clip/model.py:192\u001b[0m, in \u001b[0;36mResidualAttentionBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m    191\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_1(x))\n\u001b[0;32m--> 192\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmlp(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mln_2(x))\n\u001b[1;32m    193\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
            "File \u001b[0;32m~/miniconda3/envs/diffusion/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/miniconda3/envs/diffusion/lib/python3.9/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
            "File \u001b[0;32m~/miniconda3/envs/diffusion/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m/disco-diffusion/CLIP/clip/model.py:169\u001b[0m, in \u001b[0;36mQuickGELU.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m--> 169\u001b[0m     \u001b[39mreturn\u001b[39;00m x \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39msigmoid(\u001b[39m1.702\u001b[39;49m \u001b[39m*\u001b[39;49m x)\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 0; 10.76 GiB total capacity; 8.61 GiB already allocated; 60.00 MiB free; 9.33 GiB reserved in total by PyTorch)"
          ]
        }
      ],
      "source": [
        "#@title Do the Run!\n",
        "#@markdown `n_batches` ignored with animation modes.\n",
        "display_rate =  10 #@param{type: 'number'}\n",
        "n_batches =  9 #@param{type: 'number'}\n",
        "display_histogram = False#@param{type: 'boolean'}\n",
        "\n",
        "#Update Model Settings\n",
        "timestep_respacing = f'ddim{steps}'\n",
        "diffusion_steps = (1000//steps)*steps if steps < 1000 else steps\n",
        "model_config.update({\n",
        "    'timestep_respacing': timestep_respacing,\n",
        "    'diffusion_steps': diffusion_steps,\n",
        "})\n",
        "\n",
        "batch_size = 1 \n",
        "\n",
        "def move_files(start_num, end_num, old_folder, new_folder):\n",
        "    for i in range(start_num, end_num):\n",
        "        old_file = old_folder + f'/{batch_name}({batchNum})_{i:04}.png'\n",
        "        new_file = new_folder + f'/{batch_name}({batchNum})_{i:04}.png'\n",
        "        os.rename(old_file, new_file)\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "\n",
        "resume_run = False #@param{type: 'boolean'}\n",
        "run_to_resume = 'latest' #@param{type: 'string'}\n",
        "resume_from_frame = 'latest' #@param{type: 'string'}\n",
        "retain_overwritten_frames = False #@param{type: 'boolean'}\n",
        "if retain_overwritten_frames:\n",
        "    retainFolder = f'{batchFolder}/retained'\n",
        "    createPath(retainFolder)\n",
        "\n",
        "skip_step_ratio = int(frames_skip_steps.rstrip(\"%\")) / 100\n",
        "calc_frames_skip_steps = math.floor(steps * skip_step_ratio)\n",
        "\n",
        "\n",
        "if steps <= calc_frames_skip_steps:\n",
        "  sys.exit(\"ERROR: You can't skip more steps than your total steps\")\n",
        "\n",
        "if resume_run:\n",
        "    if run_to_resume == 'latest':\n",
        "        try:\n",
        "            batchNum\n",
        "        except:\n",
        "            batchNum = len(glob(f\"{batchFolder}/{batch_name}(*)_settings.txt\"))-1\n",
        "    else:\n",
        "        batchNum = int(run_to_resume)\n",
        "    if resume_from_frame == 'latest':\n",
        "        start_frame = len(glob(batchFolder+f\"/{batch_name}({batchNum})_*.png\"))\n",
        "        if animation_mode != '3D' and turbo_mode == True and start_frame > turbo_preroll and start_frame % int(turbo_steps) != 0:\n",
        "            start_frame = start_frame - (start_frame % int(turbo_steps))\n",
        "    else:\n",
        "        start_frame = int(resume_from_frame)+1\n",
        "        if animation_mode != '3D' and turbo_mode == True and start_frame > turbo_preroll and start_frame % int(turbo_steps) != 0:\n",
        "            start_frame = start_frame - (start_frame % int(turbo_steps))\n",
        "        if retain_overwritten_frames is True:\n",
        "            existing_frames = len(glob(batchFolder+f\"/{batch_name}({batchNum})_*.png\"))\n",
        "            frames_to_save = existing_frames - start_frame\n",
        "            print(f'Moving {frames_to_save} frames to the Retained folder')\n",
        "            move_files(start_frame, existing_frames, batchFolder, retainFolder)\n",
        "else:\n",
        "    start_frame = 0\n",
        "    batchNum = len(glob(batchFolder+\"/*.txt\"))\n",
        "    while os.path.isfile(f\"{batchFolder}/{batch_name}({batchNum})_settings.txt\") or os.path.isfile(f\"{batchFolder}/{batch_name}-{batchNum}_settings.txt\"):\n",
        "        batchNum += 1\n",
        "\n",
        "print(f'Starting Run: {batch_name}({batchNum}) at frame {start_frame}')\n",
        "\n",
        "if set_seed == 'random_seed':\n",
        "    random.seed()\n",
        "    seed = random.randint(0, 2**32)\n",
        "    # print(f'Using seed: {seed}')\n",
        "else:\n",
        "    seed = int(set_seed)\n",
        "\n",
        "args = {\n",
        "    'batchNum': batchNum,\n",
        "    'prompts_series':split_prompts(text_prompts) if text_prompts else None,\n",
        "    'image_prompts_series':split_prompts(image_prompts) if image_prompts else None,\n",
        "    'seed': seed,\n",
        "    'display_rate':display_rate,\n",
        "    'n_batches':n_batches if animation_mode == 'None' else 1,\n",
        "    'batch_size':batch_size,\n",
        "    'batch_name': batch_name,\n",
        "    'steps': steps,\n",
        "    'diffusion_sampling_mode': diffusion_sampling_mode,\n",
        "    'width_height': width_height,\n",
        "    'clip_guidance_scale': clip_guidance_scale,\n",
        "    'tv_scale': tv_scale,\n",
        "    'range_scale': range_scale,\n",
        "    'sat_scale': sat_scale,\n",
        "    'sat_scale_buffer': sat_scale_buffer,\n",
        "    'cutn_batches': cutn_batches,\n",
        "    'soft_limiter_on': soft_limiter_on,\n",
        "    'soft_limiter_knee': soft_limiter_knee,\n",
        "    'init_image': init_image,\n",
        "    'init_scale': init_scale,\n",
        "    'skip_steps': skip_steps,\n",
        "    'skip_end_steps': skip_end_steps,\n",
        "    'side_x': side_x,\n",
        "    'side_y': side_y,\n",
        "    'timestep_respacing': timestep_respacing,\n",
        "    'diffusion_steps': diffusion_steps,\n",
        "    'animation_mode': animation_mode,\n",
        "    'video_init_path': video_init_path,\n",
        "    'extract_nth_frame': extract_nth_frame,\n",
        "    'video_init_seed_continuity': video_init_seed_continuity,\n",
        "    'key_frames': key_frames,\n",
        "    'max_frames': max_frames if animation_mode != \"None\" else 1,\n",
        "    'interp_spline': interp_spline,\n",
        "    'start_frame': start_frame,\n",
        "    'angle': angle,\n",
        "    'zoom': zoom,\n",
        "    'translation_x': translation_x,\n",
        "    'translation_y': translation_y,\n",
        "    'translation_z': translation_z,\n",
        "    'rotation_3d_x': rotation_3d_x,\n",
        "    'rotation_3d_y': rotation_3d_y,\n",
        "    'rotation_3d_z': rotation_3d_z,\n",
        "    'midas_depth_model': midas_depth_model,\n",
        "    'midas_weight': midas_weight,\n",
        "    'near_plane': near_plane,\n",
        "    'far_plane': far_plane,\n",
        "    'fov': fov,\n",
        "    'padding_mode': padding_mode,\n",
        "    'sampling_mode': sampling_mode,\n",
        "    'angle_series':angle_series,\n",
        "    'zoom_series':zoom_series,\n",
        "    'translation_x_series':translation_x_series,\n",
        "    'translation_y_series':translation_y_series,\n",
        "    'translation_z_series':translation_z_series,\n",
        "    'rotation_3d_x_series':rotation_3d_x_series,\n",
        "    'rotation_3d_y_series':rotation_3d_y_series,\n",
        "    'rotation_3d_z_series':rotation_3d_z_series,\n",
        "    'frames_scale': frames_scale,\n",
        "    'calc_frames_skip_steps': calc_frames_skip_steps,\n",
        "    'skip_step_ratio': skip_step_ratio,\n",
        "    'frames_skip_end_steps': frames_skip_end_steps,\n",
        "    'text_prompts': text_prompts,\n",
        "    'image_prompts': image_prompts,\n",
        "    'cut_overview': eval(cut_overview),\n",
        "    'cut_innercut': eval(cut_innercut),\n",
        "    'cut_ic_pow': cut_ic_pow,\n",
        "    'cut_icgray_p': eval(cut_icgray_p),\n",
        "    'intermediate_saves': intermediate_saves,\n",
        "    'intermediates_in_subfolder': intermediates_in_subfolder,\n",
        "    'steps_per_checkpoint': steps_per_checkpoint,\n",
        "    'perlin_init': perlin_init,\n",
        "    'perlin_mode': perlin_mode,\n",
        "    'set_seed': set_seed,\n",
        "    'eta': eta,\n",
        "    'clamp_grad': clamp_grad,\n",
        "    'clamp_max': clamp_max,\n",
        "    'skip_augs': skip_augs,\n",
        "    'randomize_class': randomize_class,\n",
        "    'clip_denoised': clip_denoised,\n",
        "    'fuzzy_prompt': fuzzy_prompt,\n",
        "    'rand_mag': rand_mag,\n",
        "    'display_histogram': display_histogram,\n",
        "    'noise_injector': noise_injector,\n",
        "    'noise_injector_threshold': noise_injector_threshold,\n",
        "    'noise_injector_mix': noise_injector_mix,\n",
        "    'noise_injector_px_size': noise_injector_px_size,\n",
        "    'noise_injector_spice': noise_injector_spice,\n",
        "    'noise_injector_mask_feather': noise_injector_mask_feather,\n",
        "    'noise_injector_sigma': noise_injector_sigma,\n",
        "    'copy_palette': copy_palette,\n",
        "    'copy_palette_image': copy_palette_image,\n",
        "}\n",
        "\n",
        "args = SimpleNamespace(**args)\n",
        "\n",
        "print('Prepping model...')\n",
        "model, diffusion = create_model_and_diffusion(**model_config)\n",
        "model.load_state_dict(torch.load(f'{model_path}/{diffusion_model}.pt', map_location='cpu'))\n",
        "model.requires_grad_(False).eval().to(device)\n",
        "for name, param in model.named_parameters():\n",
        "    if 'qkv' in name or 'norm' in name or 'proj' in name:\n",
        "        param.requires_grad_()\n",
        "if model_config['use_fp16']:\n",
        "    model.convert_to_fp16()\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "try:\n",
        "    do_run()\n",
        "except KeyboardInterrupt:\n",
        "    pass\n",
        "finally:\n",
        "    print('Seed used:', seed)\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CreateVidTop"
      },
      "source": [
        "# 5. Create the video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "CreateVid"
      },
      "outputs": [],
      "source": [
        "# @title ### **Create video**\n",
        "#@markdown Video file will save in the same folder as your images.\n",
        "\n",
        "skip_video_for_run_all = True #@param {type: 'boolean'}\n",
        "\n",
        "if skip_video_for_run_all == True:\n",
        "   print('Skipping video creation, uncheck skip_video_for_run_all if you want to run it')\n",
        "\n",
        "else:\n",
        "    # import subprocess in case this cell is run without the above cells\n",
        "    import subprocess\n",
        "    from base64 import b64encode\n",
        "\n",
        "    latest_run = batchNum\n",
        "\n",
        "    folder = batch_name #@param\n",
        "    run = latest_run #@param\n",
        "    final_frame = 'final_frame'\n",
        "\n",
        "\n",
        "    init_frame = 1#@param {type:\"number\"} This is the frame where the video will start\n",
        "    last_frame = final_frame#@param {type:\"number\"} You can change i to the number of the last frame you want to generate. It will raise an error if that number of frames does not exist.\n",
        "    fps = 12#@param {type:\"number\"}\n",
        "    # view_video_in_cell = True #@param {type: 'boolean'}\n",
        "\n",
        "    frames = []\n",
        "    # tqdm.write('Generating video...')\n",
        "\n",
        "    if last_frame == 'final_frame':\n",
        "        last_frame = len(glob(batchFolder+f\"/{folder}({run})_*.png\"))\n",
        "        print(f'Total frames: {last_frame}')\n",
        "\n",
        "    image_path = f\"{outDirPath}/{folder}/{folder}({run})_%04d.png\"\n",
        "    filepath = f\"{outDirPath}/{folder}/{folder}({run}).mp4\"\n",
        "\n",
        "\n",
        "    cmd = [\n",
        "        'ffmpeg',\n",
        "        '-y',\n",
        "        '-vcodec',\n",
        "        'png',\n",
        "        '-r',\n",
        "        str(fps),\n",
        "        '-start_number',\n",
        "        str(init_frame),\n",
        "        '-i',\n",
        "        image_path,\n",
        "        '-frames:v',\n",
        "        str(last_frame+1),\n",
        "        '-c:v',\n",
        "        'libx264',\n",
        "        '-vf',\n",
        "        f'fps={fps}',\n",
        "        '-pix_fmt',\n",
        "        'yuv420p',\n",
        "        '-crf',\n",
        "        '17',\n",
        "        '-preset',\n",
        "        'veryslow',\n",
        "        filepath\n",
        "    ]\n",
        "\n",
        "    process = subprocess.Popen(cmd, cwd=f'{batchFolder}', stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "    stdout, stderr = process.communicate()\n",
        "    if process.returncode != 0:\n",
        "        print(stderr)\n",
        "        raise RuntimeError(stderr)\n",
        "    else:\n",
        "        print(\"The video is ready and saved to the images folder\")\n",
        "\n",
        "    # if view_video_in_cell:\n",
        "    #     mp4 = open(filepath,'rb').read()\n",
        "    #     data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "    #     display.HTML(f'<video width=400 controls><source src=\"{data_url}\" type=\"video/mp4\"></video>')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "anaconda-cloud": {},
    "colab": {
      "collapsed_sections": [
        "CreditsChTop",
        "TutorialTop"
      ],
      "machine_shape": "hm",
      "name": "Pixel Art Diffusion v2.01",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('diffusion': conda)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "2b827711131292dbf35f4643d224dd6f5c93b39a6a53afe508faf377032d9a33"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
